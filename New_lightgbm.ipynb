{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import statsmodels\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from pandas import Timestamp\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor \n",
    "from xgboost.sklearn import XGBClassifier # sklearnâ€™s Grid Search with parallel processing\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('DF_File_sample.csv')\n",
    "data['SKU_Customer'] = data['DemandCustomer'] + '_' + data['SKU10']\n",
    "data_labels = data['SKU_Customer'] \n",
    "data.drop(['DemandCustomer', 'SKU10'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featcher engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add avg by SKU_Customer and sales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Year</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Month_No</th>\n",
       "      <th>Week_No</th>\n",
       "      <th>Sales</th>\n",
       "      <th>W_Nielsen</th>\n",
       "      <th>SKU_Customer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ARW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALL OTHERS - US_62338-91101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ARW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALL OTHERS - US_62338-91101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ARW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2527.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALL OTHERS - US_62338-91101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ARW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALL OTHERS - US_62338-91101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ARW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALL OTHERS - US_62338-91101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Brand  Year  Quarter  Month_No  Week_No   Sales  W_Nielsen  \\\n",
       "0   ARW  2015        1         1        1   205.0        0.0   \n",
       "1   ARW  2015        1         1        2  2202.0        0.0   \n",
       "2   ARW  2015        1         1        3  2527.0        0.0   \n",
       "3   ARW  2015        1         1        4   680.0        0.0   \n",
       "4   ARW  2015        1         2        5  1149.0        0.0   \n",
       "\n",
       "                  SKU_Customer  \n",
       "0  ALL OTHERS - US_62338-91101  \n",
       "1  ALL OTHERS - US_62338-91101  \n",
       "2  ALL OTHERS - US_62338-91101  \n",
       "3  ALL OTHERS - US_62338-91101  \n",
       "4  ALL OTHERS - US_62338-91101  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['SKU_Customer_month'] = data.groupby(['SKU_Customer', 'Month_No']).Sales.transform('mean')\n",
    "data['Brand_month'] = data.groupby(['Brand', 'Month_No']).Sales.transform('mean')\n",
    "data['Brand_SKU_Customer'] = data.groupby(['Brand', 'SKU_Customer']).Sales.transform('mean')\n",
    "data['Brand_SKU_Customer_month'] = data.groupby(['Brand', 'SKU_Customer', 'Month_No']).Sales.transform('mean')\n",
    "\n",
    "# and by weeks\n",
    "data['SKU_Customer_month_Week_No'] = data.groupby(['SKU_Customer', 'Month_No', 'Week_No']).Sales.transform('mean')\n",
    "data['Brand_month_Week_No'] = data.groupby(['Brand', 'Month_No', 'Week_No']).Sales.transform('mean')\n",
    "data['Brand_SKU_Customer_Week_No'] = data.groupby(['Brand', 'SKU_Customer', 'Week_No']).Sales.transform('mean')\n",
    "data['Brand_SKU_Customer_month_Week_No'] = data.groupby(['Brand', 'SKU_Customer', 'Month_No', 'Week_No']).Sales.transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Year</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Month_No</th>\n",
       "      <th>Week_No</th>\n",
       "      <th>Sales</th>\n",
       "      <th>W_Nielsen</th>\n",
       "      <th>SKU_Customer</th>\n",
       "      <th>SKU_Customer_month</th>\n",
       "      <th>Brand_month</th>\n",
       "      <th>Brand_SKU_Customer</th>\n",
       "      <th>Brand_SKU_Customer_month</th>\n",
       "      <th>SKU_Customer_month_Week_No</th>\n",
       "      <th>Brand_month_Week_No</th>\n",
       "      <th>Brand_SKU_Customer_Week_No</th>\n",
       "      <th>Brand_SKU_Customer_month_Week_No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ARW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>205.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALL OTHERS - US_62338-91101</td>\n",
       "      <td>823.958333</td>\n",
       "      <td>396.252299</td>\n",
       "      <td>885.117834</td>\n",
       "      <td>823.958333</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>220.386207</td>\n",
       "      <td>393.000000</td>\n",
       "      <td>393.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ARW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2202.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALL OTHERS - US_62338-91101</td>\n",
       "      <td>823.958333</td>\n",
       "      <td>396.252299</td>\n",
       "      <td>885.117834</td>\n",
       "      <td>823.958333</td>\n",
       "      <td>919.333333</td>\n",
       "      <td>453.778161</td>\n",
       "      <td>919.333333</td>\n",
       "      <td>919.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ARW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2527.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALL OTHERS - US_62338-91101</td>\n",
       "      <td>823.958333</td>\n",
       "      <td>396.252299</td>\n",
       "      <td>885.117834</td>\n",
       "      <td>823.958333</td>\n",
       "      <td>1032.000000</td>\n",
       "      <td>388.101149</td>\n",
       "      <td>1032.000000</td>\n",
       "      <td>1032.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ARW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>680.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALL OTHERS - US_62338-91101</td>\n",
       "      <td>823.958333</td>\n",
       "      <td>396.252299</td>\n",
       "      <td>885.117834</td>\n",
       "      <td>823.958333</td>\n",
       "      <td>951.500000</td>\n",
       "      <td>522.743678</td>\n",
       "      <td>951.500000</td>\n",
       "      <td>951.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ARW</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ALL OTHERS - US_62338-91101</td>\n",
       "      <td>1049.750000</td>\n",
       "      <td>429.921073</td>\n",
       "      <td>885.117834</td>\n",
       "      <td>1049.750000</td>\n",
       "      <td>1094.666667</td>\n",
       "      <td>365.872414</td>\n",
       "      <td>1094.666667</td>\n",
       "      <td>1094.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Brand  Year  Quarter  Month_No  Week_No   Sales  W_Nielsen  \\\n",
       "0   ARW  2015        1         1        1   205.0        0.0   \n",
       "1   ARW  2015        1         1        2  2202.0        0.0   \n",
       "2   ARW  2015        1         1        3  2527.0        0.0   \n",
       "3   ARW  2015        1         1        4   680.0        0.0   \n",
       "4   ARW  2015        1         2        5  1149.0        0.0   \n",
       "\n",
       "                  SKU_Customer  SKU_Customer_month  Brand_month  \\\n",
       "0  ALL OTHERS - US_62338-91101          823.958333   396.252299   \n",
       "1  ALL OTHERS - US_62338-91101          823.958333   396.252299   \n",
       "2  ALL OTHERS - US_62338-91101          823.958333   396.252299   \n",
       "3  ALL OTHERS - US_62338-91101          823.958333   396.252299   \n",
       "4  ALL OTHERS - US_62338-91101         1049.750000   429.921073   \n",
       "\n",
       "   Brand_SKU_Customer  Brand_SKU_Customer_month  SKU_Customer_month_Week_No  \\\n",
       "0          885.117834                823.958333                  393.000000   \n",
       "1          885.117834                823.958333                  919.333333   \n",
       "2          885.117834                823.958333                 1032.000000   \n",
       "3          885.117834                823.958333                  951.500000   \n",
       "4          885.117834               1049.750000                 1094.666667   \n",
       "\n",
       "   Brand_month_Week_No  Brand_SKU_Customer_Week_No  \\\n",
       "0           220.386207                  393.000000   \n",
       "1           453.778161                  919.333333   \n",
       "2           388.101149                 1032.000000   \n",
       "3           522.743678                  951.500000   \n",
       "4           365.872414                 1094.666667   \n",
       "\n",
       "   Brand_SKU_Customer_month_Week_No  \n",
       "0                        393.000000  \n",
       "1                        919.333333  \n",
       "2                       1032.000000  \n",
       "3                        951.500000  \n",
       "4                       1094.666667  "
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var_ind = [data.columns.get_loc(c) for c in data.columns if data.loc[:, c].dtypes=='object']\n",
    "cat_var_names = data.columns.difference(cat_var_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "for i in cat_var:\n",
    "    data.iloc[:, i] = le.fit_transform(data.iloc[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat_var:\n",
    "    data.iloc[:, c] = pd.Categorical(data.iloc[:,c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_OH = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_OH.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### agg sales by month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['year_week'] = data['Year'].astype(str) + '-' + data['Week_No'].astype(str)\n",
    "data['pre_date'] = data['year_week'].apply(lambda x: datetime.datetime.strptime(x + '-4',  \"%G-%V-%w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_null = data.groupby('pre_date').Sales.sum().loc[lambda x: x == 0].sort_values().index[0]\n",
    "data = data[data.pre_date < first_null]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_26_week = pd.Series(sorted(data['pre_date'].unique())).iloc[-26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_34_week = pd.Series(sorted(data['pre_date'].unique())).iloc[-34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "train = data[data['pre_date'] < first_34_week]\n",
    "test = data[data['pre_date'] >= first_26_week]\n",
    "data.drop(['pre_date', 'year_week'], axis =1, inplace = True)\n",
    "train.drop(['pre_date', 'year_week'], axis =1, inplace = True)\n",
    "test.drop(['pre_date', 'year_week'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "test0 = test.copy()\n",
    "test0['SKU_Customer'] = test0.merge(data_labels, left_index=True, right_index=True)['SKU_Customer']\n",
    "test['Sales'] = test0.groupby(['SKU_Customer', 'Year', 'Month_No']).Sales.transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# for LE\n",
    "test0 = test.copy()\n",
    "test['Sales'] = test0.groupby(['SKU_Customer', 'Year', 'Month_No']).Sales.transform('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.loc[:, train.columns!='Sales']\n",
    "X_test = test.loc[:, test.columns!='Sales']\n",
    "y_train = train['Sales']\n",
    "y_test = test['Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_scale(scaling_type = 'norm', X_train = X_train, X_test = X_test):\n",
    "    norm_scaler = preprocessing.StandardScaler()\n",
    "    min_max_scaler = preprocessing.MinMaxScaler() # [0, 1]\n",
    "    max_abs_scaler = preprocessing.MaxAbsScaler() # [-1, 1]\n",
    "    if scaling_type == 'norm':\n",
    "        X_train_scaled = norm_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = norm_scaler.transform(X_test)\n",
    "    elif scaling_type == 'min_max':\n",
    "        X_train_scaled = min_max_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = min_max_scaler.transform(X_test)\n",
    "    else: \n",
    "        X_train_scaled = max_abs_scaler.fit_transform(X_train)\n",
    "        X_test_scaled = max_abs_scaler.transform(X_test) \n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale by column(for categorical variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_by_col(df):\n",
    "    for var in df.select_dtypes(['number']).columns:\n",
    "        norm_scaler = preprocessing.StandardScaler()\n",
    "        df[var] = norm_scaler.fit_transform(df[var].values.reshape(-1,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_by_LE(df, cat_var_names = cat_var_names):\n",
    "    cat_var_names.append(y_train.name)\n",
    "    for var in train.columns.difference(cat_var_names):\n",
    "        norm_scaler = preprocessing.StandardScaler()\n",
    "        df[var] = norm_scaler.fit_transform(df[var].values.reshape(-1,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_var_names = ['Brand', 'SKU_Customer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled = scale_by_LE(X_train)\n",
    "X_test_scaled = scale_by_LE(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval func:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mape by every row\n",
    "def mape(y, y_pred): \n",
    "    Data = {'y': np.array(y),\n",
    "            'y_pred': np.array(y_pred)}\n",
    "    Data['MAPE'] = np.where((Data['y_pred'] == 0)&(Data['y'] == 0), 0, \\\n",
    "                           np.where((Data['y_pred'] != 0)&(Data['y']==0),1, np.abs(Data['y_pred']-Data['y'])*100/Data['y']))\n",
    "    \n",
    "    return Data['MAPE'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs(y_true - y_pred)) * 100 / np.mean(np.abs(y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape_by_month(y_true, y_pred): \n",
    "    test2 = test.copy()\n",
    "    test2['Sales_pred'] = y_pred\n",
    "    test2['SKU_Customer'] = test2.merge(data_labels, left_index=True, right_index=True)['SKU_Customer']\n",
    "    test2['Sales_pred'] = test2.groupby(['Year', 'Month_No', 'SKU_Customer']).Sales_pred.transform('mean')\n",
    "    return mape(test['Sales'], test2['Sales_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(np.mean(np.square(y - y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_by_month(y_true, y_pred): \n",
    "    test2 = test.copy()\n",
    "    test2['Sales_pred'] = y_pred\n",
    "    test2['SKU_Customer'] = test2.merge(data_labels, left_index=True, right_index=True)['SKU_Customer']\n",
    "    test2['Sales_pred'] = test2.groupby(['SKU_Customer', 'Year', 'Month_No']).Sales_pred.transform('mean')\n",
    "    return rmse(test['Sales'], test2['Sales_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MAD_by_month(y, y_pred):\n",
    "    Data = {'y': np.array(y),\n",
    "            'y_pred': np.array(y_pred)}\n",
    "    \n",
    "    test2 = test.copy()\n",
    "    test2['y_pred'] = Data['y_pred']\n",
    "    test2['SKU_Customer'] = test2.merge(data_labels, left_index=True, right_index=True)['SKU_Customer']\n",
    "    test2['y_pred'] = test2.groupby(['Year', 'Month_No', 'SKU_Customer']).y_pred.transform('mean')\n",
    "    Data['y_pred'] = test2['y_pred']\n",
    "    \n",
    "    Data['MAD'] = np.where((Data['y_pred'] == 0)&(Data['y'] == 0), 0, \\\n",
    "                           np.where((Data['y_pred'] == 0)&(Data['y']!=0),1, np.abs(Data['y_pred']-Data['y'])/Data['y_pred']))\n",
    "    return Data['MAD'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge mape: 78.26210458316488 {'alpha': 0.99, 'fit_intercept': True}\n",
      "ridge rmse: 4449.260697701879\n",
      "ridge MAD: 2.779961216820889\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0, 1, 0.01)\n",
    "fit_interceptOptions = ([True, False])\n",
    "\n",
    "model = Ridge() \n",
    "scorer = make_scorer(mape, greater_is_better=False)\n",
    "\n",
    "ridge_cv = GridSearchCV(model, param_grid=dict(alpha=alphas, fit_intercept=fit_interceptOptions), scoring = scorer, n_jobs=-2)\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"ridge mape:\", mape_by_month(y_test, ridge_cv.predict(X_test_scaled)), ridge_cv.best_params_)\n",
    "print(\"ridge rmse:\", rmse_by_month(y_test, ridge_cv.predict(X_test_scaled)))\n",
    "print(\"ridge MAD:\", MAD_by_month(y_test, ridge_cv.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso mape: 43.844628487904465 {'alpha': 0.29, 'fit_intercept': True}\n",
      "lasso rmse: 582.8834310653247\n",
      "ridge rmse: 0.6283063149358187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 19725414.66720009, tolerance: 1416151.2387218715\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "alphas = np.arange(0, 1, 0.01)\n",
    "fit_interceptOptions = ([True, False])\n",
    "\n",
    "model = Lasso() \n",
    "scorer = make_scorer(mape, greater_is_better=False)\n",
    "\n",
    "lasso_cv = GridSearchCV(model, param_grid=dict(alpha=alphas, fit_intercept=fit_interceptOptions), scoring = scorer, n_jobs=-2)\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"lasso mape:\", mape_by_month(y_test, lasso_cv.predict(X_test_scaled)), lasso_cv.best_params_)\n",
    "print(\"lasso rmse:\", rmse_by_month(y_test, lasso_cv.predict(X_test_scaled)))\n",
    "print(\"lasso MAD:\", MAD_by_month(y_test, lasso_cv.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ElasticNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet mape: 43.89520986906346 {'alpha': 0.01, 'fit_intercept': True, 'l1_ratio': 0.8}\n",
      "ElasticNet rmse: 583.4014421480716\n",
      "ElasticNet MAD: 0.6331462398547043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 14776034.158023834, tolerance: 1416151.2387218715\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "alphas = np.logspace(-5,2,8)\n",
    "l1_ratio = [.1, .15, .2, .25, .3,.4,.5,.6,.8]\n",
    "fit_interceptOptions = ([True, False])\n",
    "\n",
    "model = ElasticNet() \n",
    "scorer = make_scorer(mape, greater_is_better=False)\n",
    "\n",
    "ElasticNet_cv = GridSearchCV(model, param_grid=dict(alpha=alphas, l1_ratio = l1_ratio, fit_intercept=fit_interceptOptions), scoring = scorer, n_jobs=-2)\n",
    "ElasticNet_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"ElasticNet mape:\", mape_by_month(y_test, ElasticNet_cv.predict(X_test_scaled)), ElasticNet_cv.best_params_)\n",
    "print(\"ElasticNet rmse:\", rmse_by_month(y_test, ElasticNet_cv.predict(X_test_scaled)))\n",
    "print(\"ElasticNet MAD:\", MAD_by_month(y_test, ElasticNet_cv.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet mape: 43.88760884080415 {'alpha': 0.009, 'fit_intercept': True, 'l1_ratio': 0.8}\n",
      "ElasticNet rmse: 583.3577674291163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 596096359.1907067, tolerance: 1416151.2387218715\n",
      "  positive)\n"
     ]
    }
   ],
   "source": [
    "alphas = [.009, .01, .015, .02,.05,.1]\n",
    "l1_ratio = [.1,.2,.3,.4,.5,.55, .6, .65,.7,.8]\n",
    "fit_interceptOptions = ([True, False])\n",
    "\n",
    "model = ElasticNet() \n",
    "scorer = make_scorer(mape, greater_is_better=False)\n",
    "\n",
    "ElasticNet_cv = GridSearchCV(model, param_grid=dict(alpha=alphas, l1_ratio = l1_ratio, fit_intercept=fit_interceptOptions), scoring = scorer, n_jobs=-2)\n",
    "ElasticNet_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"ElasticNet mape:\", mape_by_month(y_test, ElasticNet_cv.predict(X_test_scaled)), ElasticNet_cv.best_params_)\n",
    "print(\"ElasticNet rmse:\", rmse_by_month(y_test, ElasticNet_cv.predict(X_test_scaled)))\n",
    "print(\"ElasticNet MAD:\", MAD_by_month(y_test, ElasticNet_cv.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso takes the best result => tune diff scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tune diff scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### min_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso mape: 50.17694271242768 {'alpha': 10.0, 'fit_intercept': False, 'l1_ratio': 0.8}\n",
      "ElasticNet rmse: 351.0281888887466\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('min_max', X_train, X_test)\n",
    "alphas = np.logspace(-5,2,8)\n",
    "l1_ratio = [.1, .15, .2, .25, .3,.4,.5,.6,.8]\n",
    "\n",
    "model = ElasticNet()  \n",
    "scorer = make_scorer(mape, greater_is_better=False)\n",
    "\n",
    "ElasticNet_cv_mm = GridSearchCV(model, param_grid=dict(alpha=alphas, l1_ratio=l1_ratio,fit_intercept=fit_interceptOptions), scoring = scorer, n_jobs=-2)\n",
    "ElasticNet_cv_mm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Lasso mape:\", mape_by_month(y_test, ElasticNet_cv_mm.predict(X_test_scaled)), ElasticNet_cv_mm.best_params_)\n",
    "print(\"ElasticNet rmse:\", rmse_by_month(y_test, ElasticNet_cv_mm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### max_abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso mape: 62.50765658487119 {'alpha': 10.0, 'fit_intercept': False, 'l1_ratio': 0.8}\n",
      "ElasticNet rmse: 426.73585900075085\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('max_abs', X_train, X_test)\n",
    "alphas = np.logspace(-5,2,8)\n",
    "l1_ratio = [.1, .15, .2, .25, .3,.4,.5,.6,.8]\n",
    "\n",
    "model = ElasticNet()  \n",
    "scorer = make_scorer(mape, greater_is_better=False)\n",
    "\n",
    "ElasticNet_cv_ma = GridSearchCV(model, param_grid=dict(alpha=alphas, l1_ratio=l1_ratio,fit_intercept=fit_interceptOptions), scoring = scorer, n_jobs=-2)\n",
    "ElasticNet_cv_ma.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Lasso mape:\", mape_by_month(y_test, ElasticNet_cv_ma.predict(X_test_scaled)), ElasticNet_cv_mm.best_params_)\n",
    "print(\"ElasticNet rmse:\", rmse_by_month(y_test, ElasticNet_cv_ma.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR mape: 82.48684969248089 {'C': 1.5, 'epsilon': 0.3, 'gamma': 1e-07, 'kernel': 'linear'}\n",
      "SVR rmse: 4638.55357824556\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "parameters = {'kernel': ('linear', 'rbf','poly'),\n",
    "              'C':[1.5, 10],\n",
    "              'gamma': [1e-7, 1e-4],\n",
    "              'epsilon':[0.1,0.2,0.5,0.3]}\n",
    "\n",
    "model = SVR()\n",
    "scorer = make_scorer(mape, greater_is_better=False)\n",
    "svr = GridSearchCV(model, parameters, scoring = scorer, n_jobs=-2)\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"SVR mape:\", mape_by_month(y_test, svr.predict(X_test_scaled)), svr.best_params_)\n",
    "print(\"SVR rmse:\", rmse_by_month(y_test, svr.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVR mape: 82.32578475959185 {'C': 2, 'epsilon': 0.45, 'gamma': 1e-10, 'kernel': 'linear'}\n",
      "SVR rmse: 4630.3285453342405\n",
      "SVR MAD: 2.81015437625592\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "parameters = {'kernel': ['linear'],\n",
    "              'C':[1, 2, 4, 5],\n",
    "              'gamma': [1e-10, 1e-9, 1e-8, 1e-7],\n",
    "              'epsilon':[.3, .35, .45, .5]}\n",
    " \n",
    "model = SVR()\n",
    "scorer = make_scorer(mape, greater_is_better=False)\n",
    "svr = GridSearchCV(model, parameters, scoring = scorer, n_jobs=-2)\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"SVR mape:\", mape_by_month(y_test, svr.predict(X_test_scaled)), svr.best_params_)\n",
    "print(\"SVR rmse:\", rmse_by_month(y_test, svr.predict(X_test_scaled)))\n",
    "print(\"SVR MAD:\", MAD_by_month(y_test, ElasticNet_cv.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lightgbm with k-Fold:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 82.13935749134242 {'learning_rate': 0.02, 'max_depth': 7, 'n_estimators': 160, 'num_leaves': 9}\n",
      "lightgbm rmse: 4510.713280213905\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : list(range(100, 180,10)),\n",
    "    'num_leaves': list(range(7,10, 1)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [2, 4, 6, 7, 8],\n",
    "    'learning_rate': [0.01, .016, .02, .1]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "                                                  \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, n_jobs = -2)\n",
    "\n",
    "\n",
    "gbm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm.predict(X_test_scaled)), gbm.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 78.86952945136558 {'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 170, 'num_leaves': 8}\n",
      "lightgbm rmse: 4422.83835990909\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : list(range(100, 180,10)),\n",
    "    'num_leaves': list(range(7,10, 1)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [2, 4, 6, 7, 8],\n",
    "    'learning_rate': [0.01, .016, .02, .1]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "                                                  \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   cv = 7,\n",
    "                   n_jobs = -2)\n",
    "\n",
    "\n",
    "gbm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm.predict(X_test_scaled)), gbm.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 78.81297203627314 {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 170, 'num_leaves': 8}\n",
      "lightgbm rmse: 4422.940683731223\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : list(range(100, 180,10)),\n",
    "    'num_leaves': list(range(7,10, 1)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [2, 4, 6, 7, 8],\n",
    "    'learning_rate': [0.01, .016, .02, .1]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "                                                  \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   cv = 10,\n",
    "                   n_jobs = -2)\n",
    "\n",
    "\n",
    "gbm.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm.predict(X_test_scaled)), gbm.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightgbm with Group k-Fold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = X_train['Year'].astype(str) + '_' + X_train['Week_No'].astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = X_train[['Year', 'Week_No']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 77.56147635770547 {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 170, 'num_leaves': 9}\n",
      "lightgbm rmse: 4368.990213180459\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : list(range(100, 180,10)),\n",
    "    'num_leaves': list(range(7,10, 1)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [2, 4, 6, 7, 8],\n",
    "    'learning_rate': [0.01, .016, .02, .1, .2, .3]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "\n",
    "    \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm_g1 = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   n_jobs = -2,\n",
    "                   cv = GroupKFold(n_splits=5))\n",
    "\n",
    "\n",
    "gbm_g1.fit(X_train_scaled, y_train, groups=groups)\n",
    "\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm_g1.predict(X_test_scaled)), gbm_g1.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm_g1.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mape: 77.56147635770547\n",
      "XGB rmse: 4368.990213180459\n"
     ]
    }
   ],
   "source": [
    "# use parametesr into models:\n",
    "gbm_g1_result = lgb.LGBMRegressor(cat_features= cat_var, max_depth=7, n_estimators=170, learning_rate=0.2, num_leaves=9).fit(X_train_scaled, y_train)\n",
    "print(\"XGB mape:\", mape_by_month(y_test, gbm_g1_result.predict(X_test_scaled)))\n",
    "print(\"XGB rmse:\", rmse(y_test, gbm_g1_result.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 78.59512841291054 {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 170, 'num_leaves': 9}\n",
      "lightgbm rmse: 4413.7326235717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : list(range(100, 180,10)),\n",
    "    'num_leaves': list(range(7,10, 1)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [2, 4, 6, 7, 8],\n",
    "    'learning_rate': [0.01, .016, .02, .1, .2, .3]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "\n",
    "    \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   n_jobs = -2,\n",
    "                   cv = GroupKFold(n_splits=6))\n",
    "\n",
    "\n",
    "gbm.fit(X_train_scaled, y_train, groups=groups)\n",
    "\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm.predict(X_test_scaled)), gbm.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 77.96815178417387 {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 170, 'num_leaves': 8}\n",
      "lightgbm rmse: 4386.864356700242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : list(range(100, 180,10)),\n",
    "    'num_leaves': list(range(7,10, 1)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [2, 4, 6, 7, 8],\n",
    "    'learning_rate': [0.01, .016, .02, .1, .2, .3]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "\n",
    "    \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   n_jobs = -2,\n",
    "                   cv = GroupKFold(n_splits=8))\n",
    "\n",
    "\n",
    "gbm.fit(X_train_scaled, y_train, groups=groups)\n",
    "\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm.predict(X_test_scaled)), gbm.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 77.81430366609294 {'learning_rate': 0.2, 'max_depth': 6, 'n_estimators': 170, 'num_leaves': 9}\n",
      "lightgbm rmse: 4363.915162900023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : list(range(100, 180,10)),\n",
    "    'num_leaves': list(range(7,10, 1)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [2, 4, 6, 7, 8],\n",
    "    'learning_rate': [0.01, .016, .02, .1, .2, .3]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "\n",
    "    \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   n_jobs = -2,\n",
    "                   cv = GroupKFold(n_splits=12))\n",
    "\n",
    "\n",
    "gbm.fit(X_train_scaled, y_train, groups=groups)\n",
    "\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm.predict(X_test_scaled)), gbm.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 78.07847265417618 {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 170, 'num_leaves': 8}\n",
      "lightgbm rmse: 4406.06631028755\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : list(range(100, 180,10)),\n",
    "    'num_leaves': list(range(7,10, 1)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [2, 4, 6, 7, 8],\n",
    "    'learning_rate': [0.01, .016, .02, .1, .2, .3]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "\n",
    "    \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   n_jobs = -2,\n",
    "                   cv = GroupKFold(n_splits=10))\n",
    "\n",
    "\n",
    "gbm.fit(X_train_scaled, y_train, groups=groups)\n",
    "\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm.predict(X_test_scaled)), gbm.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning another parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 77.65397911725476 {'learning_rate': 0.2, 'max_depth': 7, 'n_estimators': 170, 'num_leaves': 9, 'reg_alpha': 0.1, 'reg_lambda': 0.3833333333333333}\n",
      "lightgbm rmse: 4364.7998910684055\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : [170],\n",
    "    'num_leaves': [9],\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [7],\n",
    "    'learning_rate': [.2],\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    'reg_lambda': np.linspace(0.1, 0.95, 10)}\n",
    "\n",
    "    \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   n_jobs = -2,\n",
    "                   cv = GroupKFold(n_splits=5))\n",
    "\n",
    "\n",
    "gbm.fit(X_train_scaled, y_train, groups=groups)\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm.predict(X_test_scaled)), gbm.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 77.83103272916486 {'learning_rate': 0.2, 'max_depth': 7, 'min_data_in_leaf': 10, 'n_estimators': 170, 'num_leaves': 9, 'reg_alpha': 0.7222222222222222, 'reg_lambda': 0.2777777777777778}\n",
      "lightgbm rmse: 4349.254952177309\n"
     ]
    }
   ],
   "source": [
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : [170],\n",
    "    'num_leaves': [9],\n",
    "    'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [7],\n",
    "    'learning_rate': [.2],\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    'reg_alpha': np.linspace(0.1, .9, 10),\n",
    "    'reg_lambda': np.linspace(0.1, 0.9, 10)}\n",
    "\n",
    "    \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   n_jobs = -2,\n",
    "                   cv = GroupKFold(n_splits=5))\n",
    "\n",
    "\n",
    "gbm.fit(X_train_scaled, y_train, groups=groups)\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm.predict(X_test_scaled)), gbm.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 45 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  19 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-2)]: Done 140 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-2)]: Done 343 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-2)]: Done 450 out of 450 | elapsed:  8.0min finished\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mape: 77.30885861479051 {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 200}\n",
      "XGB rmse: 4324.4381240720095\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators' : list(range(150, 400, 50)),\n",
    "    # 'num_leaves': list(range(8, 16, 4)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [8, 12, 15],\n",
    "    'learning_rate': [.3, 0.1, 0.01]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "                          \n",
    "scorer = make_scorer(mape, greater_is_better = False) \n",
    "model_xgb = xgb.XGBRegressor(objective = 'reg:squarederror')                        \n",
    "\n",
    "    \n",
    "grid_xgb = GridSearchCV(model_xgb, \n",
    "                        param_grid, \n",
    "                        scoring = scorer,\n",
    "                        n_jobs=-2,\n",
    "                        verbose=2,\n",
    "                        cv = GroupKFold(n_splits=10))\n",
    "\n",
    "grid_xgb.fit(X_train_scaled, y_train, groups = groups)\n",
    "\n",
    "print(\"XGB mape:\", mape_by_month(y_test, grid_xgb.predict(X_test_scaled)), grid_xgb.best_params_)\n",
    "print(\"XGB rmse:\", rmse(y_test, grid_xgb.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  19 tasks      | elapsed:   10.5s\n",
      "[Parallel(n_jobs=-2)]: Done  90 out of  90 | elapsed:   47.7s finished\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mape: 77.30885861479051 {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 8}\n",
      "XGB rmse: 4324.4381240720095\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators' : [200],\n",
    "    'num_leaves': list(range(8, 17, 3)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [8, 9],\n",
    "    'learning_rate': [.09, .1, 0.15]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "                          \n",
    "scorer = make_scorer(mape, greater_is_better = False) \n",
    "model_xgb_1 = xgb.XGBRegressor(objective = 'reg:squarederror')                        \n",
    "\n",
    "    \n",
    "model_xgb_1 = GridSearchCV(model_xgb, \n",
    "                        param_grid, \n",
    "                        scoring = scorer,\n",
    "                        n_jobs=-2,\n",
    "                        verbose=2,\n",
    "                        cv = GroupKFold(n_splits=5))\n",
    "\n",
    "grid_xgb_1.fit(X_train_scaled, y_train, groups = groups)\n",
    "\n",
    "print(\"XGB mape:\", mape_by_month(y_test, model_xgb_1.predict(X_test_scaled)), model_xgb_1.best_params_)\n",
    "print(\"XGB rmse:\", rmse(y_test, model_xgb_1.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  19 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-2)]: Done 140 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-2)]: Done 343 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-2)]: Done 626 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed:  9.6min finished\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mape: 77.30885861479051 {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 200, 'num_leaves': 8, 'reg_alpha': 0.28888888888888886, 'reg_lambda': 0.5722222222222222}\n",
      "XGB rmse: 4324.4381240720095\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators' : [200],\n",
    "    'num_leaves': [8],\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [8, 9],\n",
    "    'learning_rate': [.1],\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    'reg_lambda': np.linspace(0.1, 0.95, 10)}\n",
    "                          \n",
    "scorer = make_scorer(mape, greater_is_better = False) \n",
    "model_xgb_2 = xgb.XGBRegressor(objective = 'reg:squarederror')                        \n",
    "\n",
    "    \n",
    "model_xgb_2 = GridSearchCV(model_xgb, \n",
    "                        param_grid, \n",
    "                        scoring = scorer,\n",
    "                        n_jobs=-2,\n",
    "                        verbose=2,\n",
    "                        cv = GroupKFold(n_splits=5))\n",
    "\n",
    "model_xgb_2.fit(X_train_scaled, y_train, groups = groups)\n",
    "\n",
    "print(\"XGB mape:\", mape_by_month(y_test, model_xgb_2.predict(X_test_scaled)), model_xgb_2.best_params_)\n",
    "print(\"XGB rmse:\", rmse(y_test, model_xgb_2.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to use simple CV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  19 tasks      | elapsed:   10.1s\n",
      "[Parallel(n_jobs=-2)]: Done 140 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-2)]: Done 343 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-2)]: Done 626 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-2)]: Done 1000 out of 1000 | elapsed:  8.9min finished\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mape: 77.18610178943412 {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 200, 'num_leaves': 8, 'reg_alpha': 0.8555555555555555, 'reg_lambda': 0.95}\n",
      "XGB rmse: 4330.045389777716\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators' : [200],\n",
    "    'num_leaves': [8],\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [8, 9],\n",
    "    'learning_rate': [.1],\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    'reg_lambda': np.linspace(0.1, 0.95, 10)}\n",
    "                          \n",
    "scorer = make_scorer(mape, greater_is_better = False) \n",
    "model_xgb = xgb.XGBRegressor(objective = 'reg:squarederror')                        \n",
    "\n",
    "    \n",
    "grid_xgb_3 = GridSearchCV(model_xgb, \n",
    "                        param_grid, \n",
    "                        scoring = scorer,\n",
    "                        n_jobs=-2,\n",
    "                        verbose=2,\n",
    "                        cv = 5)\n",
    "\n",
    "grid_xgb_3.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"XGB mape:\", mape_by_month(y_test, grid_xgb_3.predict(X_test_scaled)), grid_xgb_3.best_params_)\n",
    "print(\"XGB rmse:\", rmse(y_test, grid_xgb_3.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mape: 77.18610178943412\n",
      "XGB rmse: 4330.045389777716\n"
     ]
    }
   ],
   "source": [
    "# use parametesr into models:\n",
    "grid_xgb_result = xgb.XGBRegressor(objective = 'reg:squarederror', max_depth=9, n_estimators=200, learning_rate=0.1, num_leaves=8, reg_alpha=0.8555555555555555, reg_lambda=0.95).fit(X_train_scaled, y_train)\n",
    "print(\"XGB mape:\", mape_by_month(y_test, grid_xgb_result.predict(X_test_scaled)))\n",
    "print(\"XGB rmse:\", rmse(y_test, grid_xgb_result.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoost:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 11 concurrent workers.\n",
      "[Parallel(n_jobs=-2)]: Done  19 tasks      | elapsed:  1.0min\n",
      "[Parallel(n_jobs=-2)]: Done 135 out of 135 | elapsed: 25.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1347.3279484\ttotal: 112ms\tremaining: 1m 51s\n",
      "1:\tlearn: 1274.1210607\ttotal: 161ms\tremaining: 1m 20s\n",
      "2:\tlearn: 1213.5243446\ttotal: 186ms\tremaining: 1m 1s\n",
      "3:\tlearn: 1155.4708507\ttotal: 236ms\tremaining: 58.7s\n",
      "4:\tlearn: 1103.7345127\ttotal: 360ms\tremaining: 1m 11s\n",
      "5:\tlearn: 1064.1289399\ttotal: 396ms\tremaining: 1m 5s\n",
      "6:\tlearn: 1026.0816398\ttotal: 513ms\tremaining: 1m 12s\n",
      "7:\tlearn: 991.4692484\ttotal: 638ms\tremaining: 1m 19s\n",
      "8:\tlearn: 960.1622645\ttotal: 761ms\tremaining: 1m 23s\n",
      "9:\tlearn: 937.3495531\ttotal: 887ms\tremaining: 1m 27s\n",
      "10:\tlearn: 915.5697075\ttotal: 1.01s\tremaining: 1m 30s\n",
      "11:\tlearn: 896.4533429\ttotal: 1.12s\tremaining: 1m 32s\n",
      "12:\tlearn: 879.4267913\ttotal: 1.24s\tremaining: 1m 34s\n",
      "13:\tlearn: 866.0112206\ttotal: 1.35s\tremaining: 1m 35s\n",
      "14:\tlearn: 852.9204125\ttotal: 1.48s\tremaining: 1m 37s\n",
      "15:\tlearn: 843.2625932\ttotal: 1.61s\tremaining: 1m 38s\n",
      "16:\tlearn: 834.4756238\ttotal: 1.73s\tremaining: 1m 40s\n",
      "17:\tlearn: 827.8824255\ttotal: 1.86s\tremaining: 1m 41s\n",
      "18:\tlearn: 822.7001800\ttotal: 1.89s\tremaining: 1m 37s\n",
      "19:\tlearn: 818.4654284\ttotal: 1.92s\tremaining: 1m 34s\n",
      "20:\tlearn: 811.8168749\ttotal: 2.05s\tremaining: 1m 35s\n",
      "21:\tlearn: 807.0603257\ttotal: 2.17s\tremaining: 1m 36s\n",
      "22:\tlearn: 799.7774289\ttotal: 2.3s\tremaining: 1m 37s\n",
      "23:\tlearn: 797.9858495\ttotal: 2.32s\tremaining: 1m 34s\n",
      "24:\tlearn: 793.5573620\ttotal: 2.44s\tremaining: 1m 35s\n",
      "25:\tlearn: 789.9545015\ttotal: 2.56s\tremaining: 1m 35s\n",
      "26:\tlearn: 787.0952602\ttotal: 2.68s\tremaining: 1m 36s\n",
      "27:\tlearn: 783.0514704\ttotal: 2.81s\tremaining: 1m 37s\n",
      "28:\tlearn: 779.0400351\ttotal: 2.93s\tremaining: 1m 38s\n",
      "29:\tlearn: 778.2201841\ttotal: 2.95s\tremaining: 1m 35s\n",
      "30:\tlearn: 774.9399647\ttotal: 3.07s\tremaining: 1m 36s\n",
      "31:\tlearn: 767.4846866\ttotal: 3.2s\tremaining: 1m 36s\n",
      "32:\tlearn: 765.3712527\ttotal: 3.33s\tremaining: 1m 37s\n",
      "33:\tlearn: 763.1936048\ttotal: 3.45s\tremaining: 1m 38s\n",
      "34:\tlearn: 761.5484845\ttotal: 3.48s\tremaining: 1m 36s\n",
      "35:\tlearn: 760.7975585\ttotal: 3.51s\tremaining: 1m 33s\n",
      "36:\tlearn: 759.0948302\ttotal: 3.63s\tremaining: 1m 34s\n",
      "37:\tlearn: 757.2555399\ttotal: 3.76s\tremaining: 1m 35s\n",
      "38:\tlearn: 756.0868791\ttotal: 3.81s\tremaining: 1m 33s\n",
      "39:\tlearn: 753.9045460\ttotal: 3.93s\tremaining: 1m 34s\n",
      "40:\tlearn: 752.9163676\ttotal: 4s\tremaining: 1m 33s\n",
      "41:\tlearn: 751.4180080\ttotal: 4.12s\tremaining: 1m 34s\n",
      "42:\tlearn: 750.5938137\ttotal: 4.19s\tremaining: 1m 33s\n",
      "43:\tlearn: 750.1614473\ttotal: 4.21s\tremaining: 1m 31s\n",
      "44:\tlearn: 749.3694508\ttotal: 4.26s\tremaining: 1m 30s\n",
      "45:\tlearn: 748.7289266\ttotal: 4.3s\tremaining: 1m 29s\n",
      "46:\tlearn: 744.0765351\ttotal: 4.35s\tremaining: 1m 28s\n",
      "47:\tlearn: 742.8484407\ttotal: 4.47s\tremaining: 1m 28s\n",
      "48:\tlearn: 741.3990383\ttotal: 4.52s\tremaining: 1m 27s\n",
      "49:\tlearn: 738.3105916\ttotal: 4.64s\tremaining: 1m 28s\n",
      "50:\tlearn: 737.6041718\ttotal: 4.68s\tremaining: 1m 27s\n",
      "51:\tlearn: 736.1517820\ttotal: 4.75s\tremaining: 1m 26s\n",
      "52:\tlearn: 735.3471486\ttotal: 4.82s\tremaining: 1m 26s\n",
      "53:\tlearn: 734.8961052\ttotal: 4.85s\tremaining: 1m 24s\n",
      "54:\tlearn: 733.6280967\ttotal: 4.92s\tremaining: 1m 24s\n",
      "55:\tlearn: 732.5434135\ttotal: 5.04s\tremaining: 1m 25s\n",
      "56:\tlearn: 727.0604667\ttotal: 5.16s\tremaining: 1m 25s\n",
      "57:\tlearn: 726.6077364\ttotal: 5.19s\tremaining: 1m 24s\n",
      "58:\tlearn: 726.4994394\ttotal: 5.21s\tremaining: 1m 23s\n",
      "59:\tlearn: 724.4099866\ttotal: 5.26s\tremaining: 1m 22s\n",
      "60:\tlearn: 722.9956288\ttotal: 5.39s\tremaining: 1m 22s\n",
      "61:\tlearn: 722.0515479\ttotal: 5.51s\tremaining: 1m 23s\n",
      "62:\tlearn: 720.1499245\ttotal: 5.64s\tremaining: 1m 23s\n",
      "63:\tlearn: 719.3101568\ttotal: 5.67s\tremaining: 1m 22s\n",
      "64:\tlearn: 717.4104121\ttotal: 5.7s\tremaining: 1m 22s\n",
      "65:\tlearn: 716.5477677\ttotal: 5.83s\tremaining: 1m 22s\n",
      "66:\tlearn: 712.6535188\ttotal: 5.97s\tremaining: 1m 23s\n",
      "67:\tlearn: 711.9384398\ttotal: 6.09s\tremaining: 1m 23s\n",
      "68:\tlearn: 711.2969072\ttotal: 6.13s\tremaining: 1m 22s\n",
      "69:\tlearn: 707.7324833\ttotal: 6.24s\tremaining: 1m 22s\n",
      "70:\tlearn: 706.9753784\ttotal: 6.36s\tremaining: 1m 23s\n",
      "71:\tlearn: 706.4562577\ttotal: 6.4s\tremaining: 1m 22s\n",
      "72:\tlearn: 705.8523850\ttotal: 6.47s\tremaining: 1m 22s\n",
      "73:\tlearn: 705.8521842\ttotal: 6.48s\tremaining: 1m 21s\n",
      "74:\tlearn: 705.8520216\ttotal: 6.49s\tremaining: 1m 20s\n",
      "75:\tlearn: 705.1829666\ttotal: 6.62s\tremaining: 1m 20s\n",
      "76:\tlearn: 704.6317440\ttotal: 6.66s\tremaining: 1m 19s\n",
      "77:\tlearn: 702.6914391\ttotal: 6.77s\tremaining: 1m 20s\n",
      "78:\tlearn: 700.4496886\ttotal: 6.9s\tremaining: 1m 20s\n",
      "79:\tlearn: 699.2460087\ttotal: 6.92s\tremaining: 1m 19s\n",
      "80:\tlearn: 698.4305809\ttotal: 7.04s\tremaining: 1m 19s\n",
      "81:\tlearn: 696.7574239\ttotal: 7.17s\tremaining: 1m 20s\n",
      "82:\tlearn: 696.3837709\ttotal: 7.24s\tremaining: 1m 19s\n",
      "83:\tlearn: 694.1543061\ttotal: 7.36s\tremaining: 1m 20s\n",
      "84:\tlearn: 693.9624431\ttotal: 7.38s\tremaining: 1m 19s\n",
      "85:\tlearn: 693.2015207\ttotal: 7.4s\tremaining: 1m 18s\n",
      "86:\tlearn: 690.9742387\ttotal: 7.53s\tremaining: 1m 19s\n",
      "87:\tlearn: 690.8925418\ttotal: 7.55s\tremaining: 1m 18s\n",
      "88:\tlearn: 690.8887190\ttotal: 7.56s\tremaining: 1m 17s\n",
      "89:\tlearn: 690.2362650\ttotal: 7.63s\tremaining: 1m 17s\n",
      "90:\tlearn: 689.5798358\ttotal: 7.76s\tremaining: 1m 17s\n",
      "91:\tlearn: 686.6612315\ttotal: 7.88s\tremaining: 1m 17s\n",
      "92:\tlearn: 686.0981651\ttotal: 8.01s\tremaining: 1m 18s\n",
      "93:\tlearn: 685.9369087\ttotal: 8.05s\tremaining: 1m 17s\n",
      "94:\tlearn: 684.6205576\ttotal: 8.18s\tremaining: 1m 17s\n",
      "95:\tlearn: 684.4048893\ttotal: 8.2s\tremaining: 1m 17s\n",
      "96:\tlearn: 684.2932943\ttotal: 8.22s\tremaining: 1m 16s\n",
      "97:\tlearn: 684.0322720\ttotal: 8.25s\tremaining: 1m 15s\n",
      "98:\tlearn: 683.9246354\ttotal: 8.27s\tremaining: 1m 15s\n",
      "99:\tlearn: 683.3885192\ttotal: 8.31s\tremaining: 1m 14s\n",
      "100:\tlearn: 683.3861244\ttotal: 8.33s\tremaining: 1m 14s\n",
      "101:\tlearn: 682.3421692\ttotal: 8.46s\tremaining: 1m 14s\n",
      "102:\tlearn: 682.0193536\ttotal: 8.53s\tremaining: 1m 14s\n",
      "103:\tlearn: 681.5691153\ttotal: 8.64s\tremaining: 1m 14s\n",
      "104:\tlearn: 681.4995703\ttotal: 8.66s\tremaining: 1m 13s\n",
      "105:\tlearn: 681.0271519\ttotal: 8.79s\tremaining: 1m 14s\n",
      "106:\tlearn: 680.6258716\ttotal: 8.91s\tremaining: 1m 14s\n",
      "107:\tlearn: 680.3758475\ttotal: 8.98s\tremaining: 1m 14s\n",
      "108:\tlearn: 680.2200526\ttotal: 9.01s\tremaining: 1m 13s\n",
      "109:\tlearn: 680.2169945\ttotal: 9.03s\tremaining: 1m 13s\n",
      "110:\tlearn: 679.9343750\ttotal: 9.1s\tremaining: 1m 12s\n",
      "111:\tlearn: 679.6389177\ttotal: 9.22s\tremaining: 1m 13s\n",
      "112:\tlearn: 679.6371916\ttotal: 9.23s\tremaining: 1m 12s\n",
      "113:\tlearn: 679.4949662\ttotal: 9.27s\tremaining: 1m 12s\n",
      "114:\tlearn: 679.2307688\ttotal: 9.38s\tremaining: 1m 12s\n",
      "115:\tlearn: 678.2673979\ttotal: 9.51s\tremaining: 1m 12s\n",
      "116:\tlearn: 678.1772492\ttotal: 9.53s\tremaining: 1m 11s\n",
      "117:\tlearn: 677.9792851\ttotal: 9.64s\tremaining: 1m 12s\n",
      "118:\tlearn: 675.9830180\ttotal: 9.77s\tremaining: 1m 12s\n",
      "119:\tlearn: 675.9145614\ttotal: 9.89s\tremaining: 1m 12s\n",
      "120:\tlearn: 674.5987342\ttotal: 10s\tremaining: 1m 12s\n",
      "121:\tlearn: 674.5262869\ttotal: 10s\tremaining: 1m 12s\n",
      "122:\tlearn: 674.2904357\ttotal: 10.1s\tremaining: 1m 11s\n",
      "123:\tlearn: 673.5165532\ttotal: 10.2s\tremaining: 1m 11s\n",
      "124:\tlearn: 673.1647278\ttotal: 10.2s\tremaining: 1m 11s\n",
      "125:\tlearn: 672.6629490\ttotal: 10.3s\tremaining: 1m 11s\n",
      "126:\tlearn: 671.4362483\ttotal: 10.4s\tremaining: 1m 11s\n",
      "127:\tlearn: 670.7159553\ttotal: 10.5s\tremaining: 1m 11s\n",
      "128:\tlearn: 670.7139745\ttotal: 10.5s\tremaining: 1m 10s\n",
      "129:\tlearn: 670.7130053\ttotal: 10.5s\tremaining: 1m 10s\n",
      "130:\tlearn: 670.3503087\ttotal: 10.6s\tremaining: 1m 10s\n",
      "131:\tlearn: 670.3196648\ttotal: 10.7s\tremaining: 1m 10s\n",
      "132:\tlearn: 670.2776733\ttotal: 10.7s\tremaining: 1m 9s\n",
      "133:\tlearn: 670.2375150\ttotal: 10.7s\tremaining: 1m 9s\n",
      "134:\tlearn: 669.7714278\ttotal: 10.8s\tremaining: 1m 9s\n",
      "135:\tlearn: 669.7704913\ttotal: 10.8s\tremaining: 1m 8s\n",
      "136:\tlearn: 669.3632074\ttotal: 11s\tremaining: 1m 9s\n",
      "137:\tlearn: 669.2149472\ttotal: 11.1s\tremaining: 1m 9s\n",
      "138:\tlearn: 669.0223236\ttotal: 11.2s\tremaining: 1m 9s\n",
      "139:\tlearn: 668.8441774\ttotal: 11.3s\tremaining: 1m 9s\n",
      "140:\tlearn: 668.2650421\ttotal: 11.5s\tremaining: 1m 9s\n",
      "141:\tlearn: 667.7588675\ttotal: 11.5s\tremaining: 1m 9s\n",
      "142:\tlearn: 667.7580591\ttotal: 11.5s\tremaining: 1m 8s\n",
      "143:\tlearn: 667.4884061\ttotal: 11.6s\tremaining: 1m 9s\n",
      "144:\tlearn: 667.1867846\ttotal: 11.7s\tremaining: 1m 9s\n",
      "145:\tlearn: 666.7305381\ttotal: 11.8s\tremaining: 1m 8s\n",
      "146:\tlearn: 664.8881559\ttotal: 11.9s\tremaining: 1m 8s\n",
      "147:\tlearn: 664.7507575\ttotal: 12s\tremaining: 1m 9s\n",
      "148:\tlearn: 664.6531364\ttotal: 12.1s\tremaining: 1m 9s\n",
      "149:\tlearn: 664.6524219\ttotal: 12.1s\tremaining: 1m 8s\n",
      "150:\tlearn: 664.6514320\ttotal: 12.2s\tremaining: 1m 8s\n",
      "151:\tlearn: 663.8164202\ttotal: 12.3s\tremaining: 1m 8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152:\tlearn: 661.1921486\ttotal: 12.4s\tremaining: 1m 8s\n",
      "153:\tlearn: 661.0672802\ttotal: 12.5s\tremaining: 1m 8s\n",
      "154:\tlearn: 661.0419959\ttotal: 12.5s\tremaining: 1m 8s\n",
      "155:\tlearn: 660.9844714\ttotal: 12.6s\tremaining: 1m 7s\n",
      "156:\tlearn: 660.8522395\ttotal: 12.7s\tremaining: 1m 8s\n",
      "157:\tlearn: 659.8305321\ttotal: 12.8s\tremaining: 1m 8s\n",
      "158:\tlearn: 659.8196826\ttotal: 12.8s\tremaining: 1m 7s\n",
      "159:\tlearn: 659.7561273\ttotal: 12.9s\tremaining: 1m 7s\n",
      "160:\tlearn: 659.6285503\ttotal: 13s\tremaining: 1m 7s\n",
      "161:\tlearn: 658.3529182\ttotal: 13.1s\tremaining: 1m 7s\n",
      "162:\tlearn: 657.8172326\ttotal: 13.2s\tremaining: 1m 7s\n",
      "163:\tlearn: 657.6632109\ttotal: 13.4s\tremaining: 1m 8s\n",
      "164:\tlearn: 657.6615097\ttotal: 13.4s\tremaining: 1m 7s\n",
      "165:\tlearn: 657.6099341\ttotal: 13.4s\tremaining: 1m 7s\n",
      "166:\tlearn: 657.4440988\ttotal: 13.5s\tremaining: 1m 7s\n",
      "167:\tlearn: 656.9495432\ttotal: 13.7s\tremaining: 1m 7s\n",
      "168:\tlearn: 656.5129453\ttotal: 13.8s\tremaining: 1m 7s\n",
      "169:\tlearn: 654.8190276\ttotal: 13.9s\tremaining: 1m 7s\n",
      "170:\tlearn: 654.3769618\ttotal: 14s\tremaining: 1m 8s\n",
      "171:\tlearn: 654.3761569\ttotal: 14s\tremaining: 1m 7s\n",
      "172:\tlearn: 654.3690211\ttotal: 14.1s\tremaining: 1m 7s\n",
      "173:\tlearn: 654.3615551\ttotal: 14.1s\tremaining: 1m 6s\n",
      "174:\tlearn: 653.7501110\ttotal: 14.2s\tremaining: 1m 7s\n",
      "175:\tlearn: 653.7473975\ttotal: 14.2s\tremaining: 1m 6s\n",
      "176:\tlearn: 651.2063683\ttotal: 14.4s\tremaining: 1m 6s\n",
      "177:\tlearn: 651.1981250\ttotal: 14.4s\tremaining: 1m 6s\n",
      "178:\tlearn: 651.1976656\ttotal: 14.4s\tremaining: 1m 5s\n",
      "179:\tlearn: 651.0125238\ttotal: 14.5s\tremaining: 1m 6s\n",
      "180:\tlearn: 650.8286086\ttotal: 14.6s\tremaining: 1m 5s\n",
      "181:\tlearn: 650.7518160\ttotal: 14.7s\tremaining: 1m 6s\n",
      "182:\tlearn: 650.6975350\ttotal: 14.7s\tremaining: 1m 5s\n",
      "183:\tlearn: 650.6110395\ttotal: 14.7s\tremaining: 1m 5s\n",
      "184:\tlearn: 649.9638055\ttotal: 14.9s\tremaining: 1m 5s\n",
      "185:\tlearn: 649.9599440\ttotal: 14.9s\tremaining: 1m 5s\n",
      "186:\tlearn: 648.9920069\ttotal: 15s\tremaining: 1m 5s\n",
      "187:\tlearn: 648.9919001\ttotal: 15s\tremaining: 1m 4s\n",
      "188:\tlearn: 648.5696761\ttotal: 15.1s\tremaining: 1m 4s\n",
      "189:\tlearn: 648.3185749\ttotal: 15.2s\tremaining: 1m 4s\n",
      "190:\tlearn: 647.2011417\ttotal: 15.3s\tremaining: 1m 4s\n",
      "191:\tlearn: 647.0726920\ttotal: 15.4s\tremaining: 1m 4s\n",
      "192:\tlearn: 646.2639240\ttotal: 15.6s\tremaining: 1m 5s\n",
      "193:\tlearn: 646.1462492\ttotal: 15.7s\tremaining: 1m 5s\n",
      "194:\tlearn: 646.1456153\ttotal: 15.7s\tremaining: 1m 4s\n",
      "195:\tlearn: 646.1449022\ttotal: 15.7s\tremaining: 1m 4s\n",
      "196:\tlearn: 646.1372033\ttotal: 15.7s\tremaining: 1m 4s\n",
      "197:\tlearn: 646.1343576\ttotal: 15.7s\tremaining: 1m 3s\n",
      "198:\tlearn: 646.0801459\ttotal: 15.9s\tremaining: 1m 3s\n",
      "199:\tlearn: 646.0393624\ttotal: 16s\tremaining: 1m 3s\n",
      "200:\tlearn: 645.8878511\ttotal: 16.1s\tremaining: 1m 4s\n",
      "201:\tlearn: 645.8810607\ttotal: 16.1s\tremaining: 1m 3s\n",
      "202:\tlearn: 645.8809267\ttotal: 16.1s\tremaining: 1m 3s\n",
      "203:\tlearn: 645.8796057\ttotal: 16.1s\tremaining: 1m 3s\n",
      "204:\tlearn: 645.8794732\ttotal: 16.2s\tremaining: 1m 2s\n",
      "205:\tlearn: 645.0817471\ttotal: 16.2s\tremaining: 1m 2s\n",
      "206:\tlearn: 645.0817067\ttotal: 16.2s\tremaining: 1m 2s\n",
      "207:\tlearn: 644.6031833\ttotal: 16.3s\tremaining: 1m 2s\n",
      "208:\tlearn: 644.6024809\ttotal: 16.3s\tremaining: 1m 1s\n",
      "209:\tlearn: 644.2661916\ttotal: 16.4s\tremaining: 1m 1s\n",
      "210:\tlearn: 644.2386435\ttotal: 16.5s\tremaining: 1m 1s\n",
      "211:\tlearn: 644.2382758\ttotal: 16.5s\tremaining: 1m 1s\n",
      "212:\tlearn: 644.2366819\ttotal: 16.5s\tremaining: 1m\n",
      "213:\tlearn: 643.9850687\ttotal: 16.6s\tremaining: 1m 1s\n",
      "214:\tlearn: 643.9840735\ttotal: 16.6s\tremaining: 1m\n",
      "215:\tlearn: 643.9777611\ttotal: 16.7s\tremaining: 1m\n",
      "216:\tlearn: 643.7141657\ttotal: 16.8s\tremaining: 1m\n",
      "217:\tlearn: 643.7140725\ttotal: 16.8s\tremaining: 1m\n",
      "218:\tlearn: 643.0569825\ttotal: 16.9s\tremaining: 1m\n",
      "219:\tlearn: 643.0384480\ttotal: 17s\tremaining: 1m\n",
      "220:\tlearn: 642.9990752\ttotal: 17.1s\tremaining: 1m\n",
      "221:\tlearn: 642.9990502\ttotal: 17.1s\tremaining: 59.9s\n",
      "222:\tlearn: 642.9957996\ttotal: 17.1s\tremaining: 59.6s\n",
      "223:\tlearn: 642.2826329\ttotal: 17.2s\tremaining: 59.7s\n",
      "224:\tlearn: 642.2555902\ttotal: 17.4s\tremaining: 59.8s\n",
      "225:\tlearn: 642.2504012\ttotal: 17.4s\tremaining: 59.5s\n",
      "226:\tlearn: 641.9607667\ttotal: 17.4s\tremaining: 59.4s\n",
      "227:\tlearn: 641.9606460\ttotal: 17.5s\tremaining: 59.1s\n",
      "228:\tlearn: 640.4655081\ttotal: 17.5s\tremaining: 59s\n",
      "229:\tlearn: 640.4475518\ttotal: 17.6s\tremaining: 59s\n",
      "230:\tlearn: 640.3065564\ttotal: 17.7s\tremaining: 58.9s\n",
      "231:\tlearn: 639.1016955\ttotal: 17.8s\tremaining: 58.9s\n",
      "232:\tlearn: 639.0964207\ttotal: 17.8s\tremaining: 58.7s\n",
      "233:\tlearn: 639.0941616\ttotal: 17.8s\tremaining: 58.4s\n",
      "234:\tlearn: 639.0470381\ttotal: 17.9s\tremaining: 58.2s\n",
      "235:\tlearn: 639.0414123\ttotal: 17.9s\tremaining: 57.9s\n",
      "236:\tlearn: 639.0411905\ttotal: 17.9s\tremaining: 57.7s\n",
      "237:\tlearn: 638.0714728\ttotal: 18s\tremaining: 57.8s\n",
      "238:\tlearn: 638.0696481\ttotal: 18.1s\tremaining: 57.5s\n",
      "239:\tlearn: 637.1130522\ttotal: 18.2s\tremaining: 57.6s\n",
      "240:\tlearn: 637.0848651\ttotal: 18.2s\tremaining: 57.3s\n",
      "241:\tlearn: 636.6690539\ttotal: 18.3s\tremaining: 57.4s\n",
      "242:\tlearn: 636.5443699\ttotal: 18.4s\tremaining: 57.5s\n",
      "243:\tlearn: 636.5438782\ttotal: 18.5s\tremaining: 57.2s\n",
      "244:\tlearn: 634.9296963\ttotal: 18.6s\tremaining: 57.3s\n",
      "245:\tlearn: 634.6968453\ttotal: 18.7s\tremaining: 57.4s\n",
      "246:\tlearn: 634.6133342\ttotal: 18.8s\tremaining: 57.4s\n",
      "247:\tlearn: 634.5608319\ttotal: 19s\tremaining: 57.5s\n",
      "248:\tlearn: 632.8040970\ttotal: 19.1s\tremaining: 57.5s\n",
      "249:\tlearn: 632.7475504\ttotal: 19.2s\tremaining: 57.6s\n",
      "250:\tlearn: 632.7450690\ttotal: 19.2s\tremaining: 57.3s\n",
      "251:\tlearn: 632.7320877\ttotal: 19.2s\tremaining: 57.1s\n",
      "252:\tlearn: 632.7299289\ttotal: 19.3s\tremaining: 56.9s\n",
      "253:\tlearn: 632.7209591\ttotal: 19.3s\tremaining: 56.7s\n",
      "254:\tlearn: 632.1883821\ttotal: 19.4s\tremaining: 56.7s\n",
      "255:\tlearn: 631.2026711\ttotal: 19.5s\tremaining: 56.8s\n",
      "256:\tlearn: 630.8599820\ttotal: 19.7s\tremaining: 56.8s\n",
      "257:\tlearn: 630.7791472\ttotal: 19.8s\tremaining: 56.9s\n",
      "258:\tlearn: 630.7621633\ttotal: 19.8s\tremaining: 56.7s\n",
      "259:\tlearn: 630.7562500\ttotal: 19.9s\tremaining: 56.6s\n",
      "260:\tlearn: 630.7542391\ttotal: 19.9s\tremaining: 56.3s\n",
      "261:\tlearn: 629.8096583\ttotal: 20s\tremaining: 56.4s\n",
      "262:\tlearn: 629.8077102\ttotal: 20s\tremaining: 56.1s\n",
      "263:\tlearn: 629.1076623\ttotal: 20.2s\tremaining: 56.2s\n",
      "264:\tlearn: 629.1074926\ttotal: 20.2s\tremaining: 56s\n",
      "265:\tlearn: 628.8801164\ttotal: 20.3s\tremaining: 56s\n",
      "266:\tlearn: 628.8800424\ttotal: 20.3s\tremaining: 55.8s\n",
      "267:\tlearn: 628.8698866\ttotal: 20.3s\tremaining: 55.5s\n",
      "268:\tlearn: 628.8696985\ttotal: 20.3s\tremaining: 55.3s\n",
      "269:\tlearn: 628.6946534\ttotal: 20.5s\tremaining: 55.4s\n",
      "270:\tlearn: 628.6945027\ttotal: 20.5s\tremaining: 55.1s\n",
      "271:\tlearn: 628.5945213\ttotal: 20.5s\tremaining: 54.9s\n",
      "272:\tlearn: 627.7484704\ttotal: 20.6s\tremaining: 54.8s\n",
      "273:\tlearn: 627.6342787\ttotal: 20.7s\tremaining: 54.9s\n",
      "274:\tlearn: 627.6342180\ttotal: 20.7s\tremaining: 54.6s\n",
      "275:\tlearn: 627.6340309\ttotal: 20.7s\tremaining: 54.4s\n",
      "276:\tlearn: 627.6339381\ttotal: 20.7s\tremaining: 54.2s\n",
      "277:\tlearn: 627.6325441\ttotal: 20.8s\tremaining: 54s\n",
      "278:\tlearn: 627.5788332\ttotal: 20.8s\tremaining: 53.8s\n",
      "279:\tlearn: 626.9045330\ttotal: 20.9s\tremaining: 53.8s\n",
      "280:\tlearn: 626.9042614\ttotal: 20.9s\tremaining: 53.6s\n",
      "281:\tlearn: 626.9032535\ttotal: 21s\tremaining: 53.4s\n",
      "282:\tlearn: 626.8922761\ttotal: 21s\tremaining: 53.2s\n",
      "283:\tlearn: 626.6731093\ttotal: 21.1s\tremaining: 53.2s\n",
      "284:\tlearn: 626.6711030\ttotal: 21.1s\tremaining: 53s\n",
      "285:\tlearn: 626.6357714\ttotal: 21.2s\tremaining: 53s\n",
      "286:\tlearn: 626.6357696\ttotal: 21.2s\tremaining: 52.8s\n",
      "287:\tlearn: 626.6200269\ttotal: 21.3s\tremaining: 52.6s\n",
      "288:\tlearn: 626.6161353\ttotal: 21.3s\tremaining: 52.4s\n",
      "289:\tlearn: 626.5421739\ttotal: 21.4s\tremaining: 52.5s\n",
      "290:\tlearn: 626.5390364\ttotal: 21.5s\tremaining: 52.3s\n",
      "291:\tlearn: 626.5131203\ttotal: 21.5s\tremaining: 52.1s\n",
      "292:\tlearn: 626.5126019\ttotal: 21.5s\tremaining: 51.9s\n",
      "293:\tlearn: 626.5109199\ttotal: 21.5s\tremaining: 51.7s\n",
      "294:\tlearn: 625.3923149\ttotal: 21.7s\tremaining: 51.8s\n",
      "295:\tlearn: 625.2239301\ttotal: 21.8s\tremaining: 51.8s\n",
      "296:\tlearn: 625.1683857\ttotal: 21.8s\tremaining: 51.6s\n",
      "297:\tlearn: 625.0957787\ttotal: 21.9s\tremaining: 51.5s\n",
      "298:\tlearn: 625.0957148\ttotal: 21.9s\tremaining: 51.3s\n",
      "299:\tlearn: 625.0941896\ttotal: 21.9s\tremaining: 51.1s\n",
      "300:\tlearn: 625.0924148\ttotal: 21.9s\tremaining: 50.9s\n",
      "301:\tlearn: 624.8002582\ttotal: 22s\tremaining: 50.9s\n",
      "302:\tlearn: 624.8002560\ttotal: 22.1s\tremaining: 50.7s\n",
      "303:\tlearn: 624.8000100\ttotal: 22.1s\tremaining: 50.5s\n",
      "304:\tlearn: 624.7936774\ttotal: 22.1s\tremaining: 50.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "305:\tlearn: 624.0540847\ttotal: 22.3s\tremaining: 50.5s\n",
      "306:\tlearn: 624.0530570\ttotal: 22.3s\tremaining: 50.3s\n",
      "307:\tlearn: 623.9908905\ttotal: 22.4s\tremaining: 50.4s\n",
      "308:\tlearn: 623.4040946\ttotal: 22.5s\tremaining: 50.4s\n",
      "309:\tlearn: 623.2396776\ttotal: 22.7s\tremaining: 50.4s\n",
      "310:\tlearn: 623.2396315\ttotal: 22.7s\tremaining: 50.2s\n",
      "311:\tlearn: 622.9016329\ttotal: 22.8s\tremaining: 50.3s\n",
      "312:\tlearn: 622.1321698\ttotal: 22.9s\tremaining: 50.3s\n",
      "313:\tlearn: 622.1301929\ttotal: 22.9s\tremaining: 50.1s\n",
      "314:\tlearn: 621.9418163\ttotal: 23.1s\tremaining: 50.2s\n",
      "315:\tlearn: 621.9413765\ttotal: 23.1s\tremaining: 50s\n",
      "316:\tlearn: 621.9413681\ttotal: 23.1s\tremaining: 49.8s\n",
      "317:\tlearn: 621.9397604\ttotal: 23.1s\tremaining: 49.6s\n",
      "318:\tlearn: 621.7611331\ttotal: 23.2s\tremaining: 49.6s\n",
      "319:\tlearn: 621.1618474\ttotal: 23.4s\tremaining: 49.6s\n",
      "320:\tlearn: 620.9460206\ttotal: 23.5s\tremaining: 49.7s\n",
      "321:\tlearn: 620.7238222\ttotal: 23.6s\tremaining: 49.7s\n",
      "322:\tlearn: 620.6645179\ttotal: 23.7s\tremaining: 49.7s\n",
      "323:\tlearn: 620.6638761\ttotal: 23.7s\tremaining: 49.6s\n",
      "324:\tlearn: 620.6269563\ttotal: 23.8s\tremaining: 49.4s\n",
      "325:\tlearn: 620.1448327\ttotal: 23.9s\tremaining: 49.4s\n",
      "326:\tlearn: 620.1178641\ttotal: 23.9s\tremaining: 49.3s\n",
      "327:\tlearn: 620.0858778\ttotal: 24s\tremaining: 49.1s\n",
      "328:\tlearn: 619.4525711\ttotal: 24.1s\tremaining: 49.1s\n",
      "329:\tlearn: 619.4215724\ttotal: 24.1s\tremaining: 49s\n",
      "330:\tlearn: 618.9607838\ttotal: 24.2s\tremaining: 49s\n",
      "331:\tlearn: 618.5280614\ttotal: 24.4s\tremaining: 49s\n",
      "332:\tlearn: 618.2804435\ttotal: 24.4s\tremaining: 48.9s\n",
      "333:\tlearn: 618.2771838\ttotal: 24.4s\tremaining: 48.7s\n",
      "334:\tlearn: 618.2717120\ttotal: 24.4s\tremaining: 48.5s\n",
      "335:\tlearn: 617.9626974\ttotal: 24.6s\tremaining: 48.5s\n",
      "336:\tlearn: 617.9165773\ttotal: 24.6s\tremaining: 48.4s\n",
      "337:\tlearn: 617.1465942\ttotal: 24.7s\tremaining: 48.5s\n",
      "338:\tlearn: 617.0790090\ttotal: 24.9s\tremaining: 48.5s\n",
      "339:\tlearn: 617.0779949\ttotal: 24.9s\tremaining: 48.3s\n",
      "340:\tlearn: 617.0707257\ttotal: 24.9s\tremaining: 48.2s\n",
      "341:\tlearn: 617.0707064\ttotal: 24.9s\tremaining: 48s\n",
      "342:\tlearn: 617.0706907\ttotal: 24.9s\tremaining: 47.8s\n",
      "343:\tlearn: 616.6208718\ttotal: 25.1s\tremaining: 47.8s\n",
      "344:\tlearn: 616.3268843\ttotal: 25.2s\tremaining: 47.8s\n",
      "345:\tlearn: 616.3128375\ttotal: 25.2s\tremaining: 47.7s\n",
      "346:\tlearn: 616.3101271\ttotal: 25.3s\tremaining: 47.5s\n",
      "347:\tlearn: 615.6826050\ttotal: 25.4s\tremaining: 47.6s\n",
      "348:\tlearn: 615.0275236\ttotal: 25.5s\tremaining: 47.6s\n",
      "349:\tlearn: 614.4573889\ttotal: 25.6s\tremaining: 47.6s\n",
      "350:\tlearn: 614.4489469\ttotal: 25.7s\tremaining: 47.5s\n",
      "351:\tlearn: 614.4331359\ttotal: 25.7s\tremaining: 47.3s\n",
      "352:\tlearn: 614.0671165\ttotal: 25.8s\tremaining: 47.3s\n",
      "353:\tlearn: 614.0626531\ttotal: 25.9s\tremaining: 47.2s\n",
      "354:\tlearn: 614.0625604\ttotal: 25.9s\tremaining: 47s\n",
      "355:\tlearn: 614.0593697\ttotal: 25.9s\tremaining: 46.9s\n",
      "356:\tlearn: 614.0575683\ttotal: 25.9s\tremaining: 46.7s\n",
      "357:\tlearn: 614.0557241\ttotal: 25.9s\tremaining: 46.5s\n",
      "358:\tlearn: 613.7050279\ttotal: 26.1s\tremaining: 46.5s\n",
      "359:\tlearn: 613.7043414\ttotal: 26.1s\tremaining: 46.4s\n",
      "360:\tlearn: 613.7005721\ttotal: 26.1s\tremaining: 46.2s\n",
      "361:\tlearn: 613.7003597\ttotal: 26.1s\tremaining: 46.1s\n",
      "362:\tlearn: 613.6935646\ttotal: 26.2s\tremaining: 46s\n",
      "363:\tlearn: 613.6930865\ttotal: 26.2s\tremaining: 45.8s\n",
      "364:\tlearn: 613.6905895\ttotal: 26.2s\tremaining: 45.7s\n",
      "365:\tlearn: 613.6419282\ttotal: 26.3s\tremaining: 45.5s\n",
      "366:\tlearn: 613.6413489\ttotal: 26.3s\tremaining: 45.3s\n",
      "367:\tlearn: 613.6061988\ttotal: 26.4s\tremaining: 45.3s\n",
      "368:\tlearn: 613.1118611\ttotal: 26.5s\tremaining: 45.3s\n",
      "369:\tlearn: 613.1081543\ttotal: 26.5s\tremaining: 45.1s\n",
      "370:\tlearn: 613.1076419\ttotal: 26.5s\tremaining: 44.9s\n",
      "371:\tlearn: 613.1075810\ttotal: 26.5s\tremaining: 44.8s\n",
      "372:\tlearn: 613.1045156\ttotal: 26.6s\tremaining: 44.6s\n",
      "373:\tlearn: 613.0995482\ttotal: 26.6s\tremaining: 44.5s\n",
      "374:\tlearn: 612.8681893\ttotal: 26.7s\tremaining: 44.5s\n",
      "375:\tlearn: 612.8462401\ttotal: 26.8s\tremaining: 44.5s\n",
      "376:\tlearn: 612.8450711\ttotal: 26.8s\tremaining: 44.3s\n",
      "377:\tlearn: 612.5694369\ttotal: 26.9s\tremaining: 44.3s\n",
      "378:\tlearn: 612.5296615\ttotal: 27s\tremaining: 44.3s\n",
      "379:\tlearn: 612.5292392\ttotal: 27s\tremaining: 44.1s\n",
      "380:\tlearn: 612.5291610\ttotal: 27s\tremaining: 43.9s\n",
      "381:\tlearn: 612.5179358\ttotal: 27.1s\tremaining: 43.8s\n",
      "382:\tlearn: 612.4386432\ttotal: 27.1s\tremaining: 43.7s\n",
      "383:\tlearn: 612.3589099\ttotal: 27.3s\tremaining: 43.7s\n",
      "384:\tlearn: 612.3507489\ttotal: 27.4s\tremaining: 43.7s\n",
      "385:\tlearn: 612.3506178\ttotal: 27.4s\tremaining: 43.6s\n",
      "386:\tlearn: 612.2216129\ttotal: 27.5s\tremaining: 43.6s\n",
      "387:\tlearn: 612.0904695\ttotal: 27.6s\tremaining: 43.5s\n",
      "388:\tlearn: 611.8188393\ttotal: 27.7s\tremaining: 43.5s\n",
      "389:\tlearn: 611.6462250\ttotal: 27.8s\tremaining: 43.5s\n",
      "390:\tlearn: 611.2822483\ttotal: 27.9s\tremaining: 43.5s\n",
      "391:\tlearn: 610.4589755\ttotal: 28s\tremaining: 43.5s\n",
      "392:\tlearn: 610.4058368\ttotal: 28.2s\tremaining: 43.5s\n",
      "393:\tlearn: 610.3569997\ttotal: 28.3s\tremaining: 43.5s\n",
      "394:\tlearn: 610.3489961\ttotal: 28.3s\tremaining: 43.4s\n",
      "395:\tlearn: 610.3366131\ttotal: 28.4s\tremaining: 43.3s\n",
      "396:\tlearn: 610.3359750\ttotal: 28.4s\tremaining: 43.1s\n",
      "397:\tlearn: 610.3277149\ttotal: 28.4s\tremaining: 43s\n",
      "398:\tlearn: 610.2735799\ttotal: 28.5s\tremaining: 43s\n",
      "399:\tlearn: 610.2716154\ttotal: 28.6s\tremaining: 42.8s\n",
      "400:\tlearn: 610.2688031\ttotal: 28.6s\tremaining: 42.7s\n",
      "401:\tlearn: 610.1230492\ttotal: 28.7s\tremaining: 42.7s\n",
      "402:\tlearn: 610.1226464\ttotal: 28.7s\tremaining: 42.5s\n",
      "403:\tlearn: 610.1048134\ttotal: 28.8s\tremaining: 42.5s\n",
      "404:\tlearn: 610.1017077\ttotal: 28.8s\tremaining: 42.3s\n",
      "405:\tlearn: 610.0679826\ttotal: 28.9s\tremaining: 42.3s\n",
      "406:\tlearn: 610.0678762\ttotal: 28.9s\tremaining: 42.2s\n",
      "407:\tlearn: 610.0476139\ttotal: 29.1s\tremaining: 42.2s\n",
      "408:\tlearn: 610.0412570\ttotal: 29.1s\tremaining: 42s\n",
      "409:\tlearn: 610.0327560\ttotal: 29.1s\tremaining: 41.9s\n",
      "410:\tlearn: 609.9817020\ttotal: 29.3s\tremaining: 41.9s\n",
      "411:\tlearn: 609.9810035\ttotal: 29.3s\tremaining: 41.8s\n",
      "412:\tlearn: 609.8155520\ttotal: 29.3s\tremaining: 41.7s\n",
      "413:\tlearn: 609.8136125\ttotal: 29.3s\tremaining: 41.5s\n",
      "414:\tlearn: 609.8108602\ttotal: 29.3s\tremaining: 41.4s\n",
      "415:\tlearn: 609.8094598\ttotal: 29.4s\tremaining: 41.2s\n",
      "416:\tlearn: 609.8091570\ttotal: 29.4s\tremaining: 41.1s\n",
      "417:\tlearn: 609.8085784\ttotal: 29.4s\tremaining: 41s\n",
      "418:\tlearn: 609.8080570\ttotal: 29.4s\tremaining: 40.8s\n",
      "419:\tlearn: 609.6540870\ttotal: 29.6s\tremaining: 40.8s\n",
      "420:\tlearn: 609.6523346\ttotal: 29.6s\tremaining: 40.7s\n",
      "421:\tlearn: 609.6256189\ttotal: 29.6s\tremaining: 40.5s\n",
      "422:\tlearn: 607.6804925\ttotal: 29.7s\tremaining: 40.5s\n",
      "423:\tlearn: 607.6803161\ttotal: 29.7s\tremaining: 40.4s\n",
      "424:\tlearn: 607.6696636\ttotal: 29.8s\tremaining: 40.3s\n",
      "425:\tlearn: 607.6694426\ttotal: 29.8s\tremaining: 40.1s\n",
      "426:\tlearn: 607.6693984\ttotal: 29.8s\tremaining: 40s\n",
      "427:\tlearn: 607.6498338\ttotal: 29.8s\tremaining: 39.9s\n",
      "428:\tlearn: 607.6496535\ttotal: 29.9s\tremaining: 39.7s\n",
      "429:\tlearn: 607.6419502\ttotal: 29.9s\tremaining: 39.7s\n",
      "430:\tlearn: 607.3411878\ttotal: 30.1s\tremaining: 39.7s\n",
      "431:\tlearn: 607.3379351\ttotal: 30.1s\tremaining: 39.6s\n",
      "432:\tlearn: 607.3378418\ttotal: 30.1s\tremaining: 39.5s\n",
      "433:\tlearn: 607.2441709\ttotal: 30.3s\tremaining: 39.5s\n",
      "434:\tlearn: 607.2423126\ttotal: 30.3s\tremaining: 39.3s\n",
      "435:\tlearn: 607.0970439\ttotal: 30.4s\tremaining: 39.3s\n",
      "436:\tlearn: 607.0970339\ttotal: 30.4s\tremaining: 39.2s\n",
      "437:\tlearn: 607.0732189\ttotal: 30.5s\tremaining: 39.1s\n",
      "438:\tlearn: 607.0730927\ttotal: 30.5s\tremaining: 39s\n",
      "439:\tlearn: 607.0730888\ttotal: 30.5s\tremaining: 38.9s\n",
      "440:\tlearn: 607.0730231\ttotal: 30.5s\tremaining: 38.7s\n",
      "441:\tlearn: 607.0718173\ttotal: 30.6s\tremaining: 38.6s\n",
      "442:\tlearn: 607.0716131\ttotal: 30.7s\tremaining: 38.5s\n",
      "443:\tlearn: 606.8956589\ttotal: 30.7s\tremaining: 38.4s\n",
      "444:\tlearn: 606.6445980\ttotal: 30.8s\tremaining: 38.4s\n",
      "445:\tlearn: 606.4680611\ttotal: 30.9s\tremaining: 38.4s\n",
      "446:\tlearn: 606.4674902\ttotal: 31s\tremaining: 38.3s\n",
      "447:\tlearn: 606.4593261\ttotal: 31s\tremaining: 38.2s\n",
      "448:\tlearn: 606.4556490\ttotal: 31.1s\tremaining: 38.2s\n",
      "449:\tlearn: 606.4555599\ttotal: 31.1s\tremaining: 38s\n",
      "450:\tlearn: 606.1928384\ttotal: 31.2s\tremaining: 38s\n",
      "451:\tlearn: 606.1926119\ttotal: 31.3s\tremaining: 38s\n",
      "452:\tlearn: 605.9355425\ttotal: 31.4s\tremaining: 38s\n",
      "453:\tlearn: 605.9287536\ttotal: 31.5s\tremaining: 37.8s\n",
      "454:\tlearn: 605.9267362\ttotal: 31.5s\tremaining: 37.7s\n",
      "455:\tlearn: 605.9262559\ttotal: 31.5s\tremaining: 37.6s\n",
      "456:\tlearn: 605.9094315\ttotal: 31.5s\tremaining: 37.5s\n",
      "457:\tlearn: 605.2069371\ttotal: 31.7s\tremaining: 37.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "458:\tlearn: 605.2069052\ttotal: 31.7s\tremaining: 37.4s\n",
      "459:\tlearn: 605.0974133\ttotal: 31.8s\tremaining: 37.3s\n",
      "460:\tlearn: 605.0279678\ttotal: 31.9s\tremaining: 37.3s\n",
      "461:\tlearn: 604.9104980\ttotal: 32.1s\tremaining: 37.3s\n",
      "462:\tlearn: 604.7144440\ttotal: 32.2s\tremaining: 37.3s\n",
      "463:\tlearn: 604.5201702\ttotal: 32.3s\tremaining: 37.3s\n",
      "464:\tlearn: 604.5200666\ttotal: 32.3s\tremaining: 37.2s\n",
      "465:\tlearn: 604.5186218\ttotal: 32.3s\tremaining: 37.1s\n",
      "466:\tlearn: 604.4915614\ttotal: 32.4s\tremaining: 37s\n",
      "467:\tlearn: 604.4779250\ttotal: 32.5s\tremaining: 37s\n",
      "468:\tlearn: 604.4779173\ttotal: 32.5s\tremaining: 36.8s\n",
      "469:\tlearn: 603.0790287\ttotal: 32.6s\tremaining: 36.8s\n",
      "470:\tlearn: 603.0789963\ttotal: 32.7s\tremaining: 36.7s\n",
      "471:\tlearn: 603.0788883\ttotal: 32.7s\tremaining: 36.6s\n",
      "472:\tlearn: 602.6639100\ttotal: 32.8s\tremaining: 36.6s\n",
      "473:\tlearn: 602.6614590\ttotal: 32.9s\tremaining: 36.5s\n",
      "474:\tlearn: 602.6297246\ttotal: 32.9s\tremaining: 36.3s\n",
      "475:\tlearn: 602.6296762\ttotal: 32.9s\tremaining: 36.2s\n",
      "476:\tlearn: 602.5790734\ttotal: 33s\tremaining: 36.2s\n",
      "477:\tlearn: 602.5690049\ttotal: 33.1s\tremaining: 36.2s\n",
      "478:\tlearn: 602.4420685\ttotal: 33.1s\tremaining: 36.1s\n",
      "479:\tlearn: 602.4420681\ttotal: 33.2s\tremaining: 35.9s\n",
      "480:\tlearn: 601.9695417\ttotal: 33.3s\tremaining: 35.9s\n",
      "481:\tlearn: 601.9676072\ttotal: 33.3s\tremaining: 35.8s\n",
      "482:\tlearn: 601.9581801\ttotal: 33.3s\tremaining: 35.7s\n",
      "483:\tlearn: 601.9531706\ttotal: 33.3s\tremaining: 35.6s\n",
      "484:\tlearn: 601.8588917\ttotal: 33.5s\tremaining: 35.5s\n",
      "485:\tlearn: 601.8306354\ttotal: 33.6s\tremaining: 35.5s\n",
      "486:\tlearn: 601.8305571\ttotal: 33.6s\tremaining: 35.4s\n",
      "487:\tlearn: 601.4535345\ttotal: 33.7s\tremaining: 35.4s\n",
      "488:\tlearn: 600.0181309\ttotal: 33.9s\tremaining: 35.4s\n",
      "489:\tlearn: 600.0174207\ttotal: 33.9s\tremaining: 35.2s\n",
      "490:\tlearn: 599.9001494\ttotal: 33.9s\tremaining: 35.2s\n",
      "491:\tlearn: 599.8773764\ttotal: 34s\tremaining: 35.1s\n",
      "492:\tlearn: 599.4623096\ttotal: 34.2s\tremaining: 35.1s\n",
      "493:\tlearn: 599.3672457\ttotal: 34.2s\tremaining: 35s\n",
      "494:\tlearn: 599.3670958\ttotal: 34.2s\tremaining: 34.9s\n",
      "495:\tlearn: 599.3659206\ttotal: 34.2s\tremaining: 34.8s\n",
      "496:\tlearn: 598.8549021\ttotal: 34.4s\tremaining: 34.8s\n",
      "497:\tlearn: 598.8091524\ttotal: 34.5s\tremaining: 34.8s\n",
      "498:\tlearn: 598.8089572\ttotal: 34.5s\tremaining: 34.7s\n",
      "499:\tlearn: 598.7916336\ttotal: 34.6s\tremaining: 34.6s\n",
      "500:\tlearn: 598.7720404\ttotal: 34.6s\tremaining: 34.4s\n",
      "501:\tlearn: 598.7698621\ttotal: 34.6s\tremaining: 34.3s\n",
      "502:\tlearn: 598.1978307\ttotal: 34.7s\tremaining: 34.3s\n",
      "503:\tlearn: 598.1951817\ttotal: 34.7s\tremaining: 34.2s\n",
      "504:\tlearn: 598.1783088\ttotal: 34.9s\tremaining: 34.2s\n",
      "505:\tlearn: 598.1534600\ttotal: 35s\tremaining: 34.1s\n",
      "506:\tlearn: 597.9844331\ttotal: 35.1s\tremaining: 34.1s\n",
      "507:\tlearn: 597.9839563\ttotal: 35.1s\tremaining: 34s\n",
      "508:\tlearn: 597.9616447\ttotal: 35.2s\tremaining: 34s\n",
      "509:\tlearn: 597.9600256\ttotal: 35.2s\tremaining: 33.9s\n",
      "510:\tlearn: 597.9252299\ttotal: 35.4s\tremaining: 33.8s\n",
      "511:\tlearn: 597.9238451\ttotal: 35.4s\tremaining: 33.7s\n",
      "512:\tlearn: 597.9236196\ttotal: 35.4s\tremaining: 33.6s\n",
      "513:\tlearn: 597.9234985\ttotal: 35.4s\tremaining: 33.5s\n",
      "514:\tlearn: 597.9054781\ttotal: 35.5s\tremaining: 33.4s\n",
      "515:\tlearn: 597.9054327\ttotal: 35.5s\tremaining: 33.3s\n",
      "516:\tlearn: 597.9011862\ttotal: 35.5s\tremaining: 33.2s\n",
      "517:\tlearn: 597.5875553\ttotal: 35.7s\tremaining: 33.2s\n",
      "518:\tlearn: 597.5875523\ttotal: 35.7s\tremaining: 33.1s\n",
      "519:\tlearn: 597.5875498\ttotal: 35.7s\tremaining: 32.9s\n",
      "520:\tlearn: 597.5209449\ttotal: 35.8s\tremaining: 32.9s\n",
      "521:\tlearn: 597.5209419\ttotal: 35.8s\tremaining: 32.8s\n",
      "522:\tlearn: 597.5209341\ttotal: 35.8s\tremaining: 32.7s\n",
      "523:\tlearn: 597.5196754\ttotal: 35.9s\tremaining: 32.6s\n",
      "524:\tlearn: 597.5194404\ttotal: 36s\tremaining: 32.5s\n",
      "525:\tlearn: 597.5105550\ttotal: 36s\tremaining: 32.4s\n",
      "526:\tlearn: 597.5006769\ttotal: 36.1s\tremaining: 32.4s\n",
      "527:\tlearn: 597.4979619\ttotal: 36.2s\tremaining: 32.4s\n",
      "528:\tlearn: 597.4977867\ttotal: 36.2s\tremaining: 32.3s\n",
      "529:\tlearn: 597.4634800\ttotal: 36.4s\tremaining: 32.2s\n",
      "530:\tlearn: 597.0072831\ttotal: 36.5s\tremaining: 32.2s\n",
      "531:\tlearn: 596.6340039\ttotal: 36.6s\tremaining: 32.2s\n",
      "532:\tlearn: 596.6301706\ttotal: 36.6s\tremaining: 32.1s\n",
      "533:\tlearn: 596.5566156\ttotal: 36.7s\tremaining: 32.1s\n",
      "534:\tlearn: 596.5566097\ttotal: 36.8s\tremaining: 31.9s\n",
      "535:\tlearn: 596.4525220\ttotal: 36.8s\tremaining: 31.9s\n",
      "536:\tlearn: 596.4525074\ttotal: 36.8s\tremaining: 31.7s\n",
      "537:\tlearn: 596.4305009\ttotal: 36.9s\tremaining: 31.7s\n",
      "538:\tlearn: 596.4281477\ttotal: 37s\tremaining: 31.6s\n",
      "539:\tlearn: 596.4256226\ttotal: 37s\tremaining: 31.5s\n",
      "540:\tlearn: 596.4256162\ttotal: 37s\tremaining: 31.4s\n",
      "541:\tlearn: 596.4241793\ttotal: 37s\tremaining: 31.3s\n",
      "542:\tlearn: 595.7896715\ttotal: 37.1s\tremaining: 31.3s\n",
      "543:\tlearn: 595.7843342\ttotal: 37.2s\tremaining: 31.2s\n",
      "544:\tlearn: 595.7843240\ttotal: 37.2s\tremaining: 31s\n",
      "545:\tlearn: 595.6777051\ttotal: 37.3s\tremaining: 31s\n",
      "546:\tlearn: 595.6729119\ttotal: 37.4s\tremaining: 31s\n",
      "547:\tlearn: 595.6591786\ttotal: 37.5s\tremaining: 30.9s\n",
      "548:\tlearn: 595.6560346\ttotal: 37.5s\tremaining: 30.8s\n",
      "549:\tlearn: 595.6042024\ttotal: 37.5s\tremaining: 30.7s\n",
      "550:\tlearn: 595.6041919\ttotal: 37.5s\tremaining: 30.6s\n",
      "551:\tlearn: 595.5972592\ttotal: 37.6s\tremaining: 30.5s\n",
      "552:\tlearn: 595.4479020\ttotal: 37.7s\tremaining: 30.5s\n",
      "553:\tlearn: 595.4473691\ttotal: 37.8s\tremaining: 30.4s\n",
      "554:\tlearn: 595.3339559\ttotal: 37.9s\tremaining: 30.4s\n",
      "555:\tlearn: 595.3011318\ttotal: 38s\tremaining: 30.4s\n",
      "556:\tlearn: 595.1229802\ttotal: 38.2s\tremaining: 30.4s\n",
      "557:\tlearn: 595.1229735\ttotal: 38.2s\tremaining: 30.2s\n",
      "558:\tlearn: 595.1225604\ttotal: 38.2s\tremaining: 30.1s\n",
      "559:\tlearn: 595.0448185\ttotal: 38.3s\tremaining: 30.1s\n",
      "560:\tlearn: 595.0447826\ttotal: 38.4s\tremaining: 30s\n",
      "561:\tlearn: 594.5691199\ttotal: 38.5s\tremaining: 30s\n",
      "562:\tlearn: 594.5616415\ttotal: 38.6s\tremaining: 30s\n",
      "563:\tlearn: 594.5616291\ttotal: 38.6s\tremaining: 29.8s\n",
      "564:\tlearn: 594.5616201\ttotal: 38.6s\tremaining: 29.7s\n",
      "565:\tlearn: 594.5609607\ttotal: 38.7s\tremaining: 29.7s\n",
      "566:\tlearn: 594.5608468\ttotal: 38.7s\tremaining: 29.6s\n",
      "567:\tlearn: 594.5301234\ttotal: 38.7s\tremaining: 29.5s\n",
      "568:\tlearn: 594.3758159\ttotal: 38.9s\tremaining: 29.4s\n",
      "569:\tlearn: 594.0227073\ttotal: 39s\tremaining: 29.4s\n",
      "570:\tlearn: 593.9832664\ttotal: 39.1s\tremaining: 29.4s\n",
      "571:\tlearn: 593.8860796\ttotal: 39.1s\tremaining: 29.3s\n",
      "572:\tlearn: 592.5695713\ttotal: 39.2s\tremaining: 29.2s\n",
      "573:\tlearn: 592.2718734\ttotal: 39.3s\tremaining: 29.2s\n",
      "574:\tlearn: 592.2584786\ttotal: 39.4s\tremaining: 29.1s\n",
      "575:\tlearn: 592.2437179\ttotal: 39.5s\tremaining: 29.1s\n",
      "576:\tlearn: 592.2436734\ttotal: 39.6s\tremaining: 29s\n",
      "577:\tlearn: 592.2421923\ttotal: 39.6s\tremaining: 28.9s\n",
      "578:\tlearn: 592.2391565\ttotal: 39.6s\tremaining: 28.8s\n",
      "579:\tlearn: 592.2383053\ttotal: 39.6s\tremaining: 28.7s\n",
      "580:\tlearn: 592.2145155\ttotal: 39.6s\tremaining: 28.6s\n",
      "581:\tlearn: 592.2115248\ttotal: 39.7s\tremaining: 28.5s\n",
      "582:\tlearn: 591.9590439\ttotal: 39.7s\tremaining: 28.4s\n",
      "583:\tlearn: 591.9576676\ttotal: 39.7s\tremaining: 28.3s\n",
      "584:\tlearn: 591.8003961\ttotal: 39.8s\tremaining: 28.2s\n",
      "585:\tlearn: 591.7819610\ttotal: 39.8s\tremaining: 28.1s\n",
      "586:\tlearn: 591.7337802\ttotal: 39.9s\tremaining: 28.1s\n",
      "587:\tlearn: 591.6081804\ttotal: 40.1s\tremaining: 28.1s\n",
      "588:\tlearn: 591.6079484\ttotal: 40.1s\tremaining: 28s\n",
      "589:\tlearn: 591.5769410\ttotal: 40.1s\tremaining: 27.9s\n",
      "590:\tlearn: 591.5743258\ttotal: 40.2s\tremaining: 27.8s\n",
      "591:\tlearn: 591.5743227\ttotal: 40.2s\tremaining: 27.7s\n",
      "592:\tlearn: 591.5394347\ttotal: 40.4s\tremaining: 27.7s\n",
      "593:\tlearn: 591.5392340\ttotal: 40.4s\tremaining: 27.6s\n",
      "594:\tlearn: 591.5392337\ttotal: 40.4s\tremaining: 27.5s\n",
      "595:\tlearn: 591.4943807\ttotal: 40.5s\tremaining: 27.5s\n",
      "596:\tlearn: 591.4323375\ttotal: 40.6s\tremaining: 27.4s\n",
      "597:\tlearn: 591.4171761\ttotal: 40.7s\tremaining: 27.4s\n",
      "598:\tlearn: 590.8743749\ttotal: 40.8s\tremaining: 27.3s\n",
      "599:\tlearn: 590.8638885\ttotal: 41s\tremaining: 27.3s\n",
      "600:\tlearn: 590.8638879\ttotal: 41s\tremaining: 27.2s\n",
      "601:\tlearn: 590.7584913\ttotal: 41.1s\tremaining: 27.2s\n",
      "602:\tlearn: 590.6306073\ttotal: 41.2s\tremaining: 27.1s\n",
      "603:\tlearn: 590.6153994\ttotal: 41.3s\tremaining: 27.1s\n",
      "604:\tlearn: 590.3081078\ttotal: 41.4s\tremaining: 27s\n",
      "605:\tlearn: 590.2389316\ttotal: 41.4s\tremaining: 26.9s\n",
      "606:\tlearn: 590.1791929\ttotal: 41.5s\tremaining: 26.8s\n",
      "607:\tlearn: 590.1791877\ttotal: 41.5s\tremaining: 26.7s\n",
      "608:\tlearn: 590.1750626\ttotal: 41.5s\tremaining: 26.6s\n",
      "609:\tlearn: 589.9884801\ttotal: 41.5s\tremaining: 26.5s\n",
      "610:\tlearn: 589.9871794\ttotal: 41.6s\tremaining: 26.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "611:\tlearn: 589.9851553\ttotal: 41.8s\tremaining: 26.5s\n",
      "612:\tlearn: 589.4569178\ttotal: 41.9s\tremaining: 26.4s\n",
      "613:\tlearn: 589.4562424\ttotal: 41.9s\tremaining: 26.3s\n",
      "614:\tlearn: 589.4532098\ttotal: 42s\tremaining: 26.3s\n",
      "615:\tlearn: 589.4370715\ttotal: 42.1s\tremaining: 26.3s\n",
      "616:\tlearn: 589.4353022\ttotal: 42.2s\tremaining: 26.2s\n",
      "617:\tlearn: 589.4352156\ttotal: 42.2s\tremaining: 26.1s\n",
      "618:\tlearn: 589.4351516\ttotal: 42.2s\tremaining: 26s\n",
      "619:\tlearn: 589.3703305\ttotal: 42.4s\tremaining: 26s\n",
      "620:\tlearn: 589.3654233\ttotal: 42.4s\tremaining: 25.9s\n",
      "621:\tlearn: 589.3605469\ttotal: 42.4s\tremaining: 25.8s\n",
      "622:\tlearn: 589.1756404\ttotal: 42.5s\tremaining: 25.7s\n",
      "623:\tlearn: 589.1756354\ttotal: 42.5s\tremaining: 25.6s\n",
      "624:\tlearn: 589.1755252\ttotal: 42.5s\tremaining: 25.5s\n",
      "625:\tlearn: 588.2963390\ttotal: 42.6s\tremaining: 25.5s\n",
      "626:\tlearn: 588.2963353\ttotal: 42.6s\tremaining: 25.4s\n",
      "627:\tlearn: 588.2926260\ttotal: 42.7s\tremaining: 25.3s\n",
      "628:\tlearn: 588.2910551\ttotal: 42.9s\tremaining: 25.3s\n",
      "629:\tlearn: 588.2910527\ttotal: 42.9s\tremaining: 25.2s\n",
      "630:\tlearn: 588.2878922\ttotal: 42.9s\tremaining: 25.1s\n",
      "631:\tlearn: 588.0843277\ttotal: 43.1s\tremaining: 25.1s\n",
      "632:\tlearn: 588.0827153\ttotal: 43.1s\tremaining: 25s\n",
      "633:\tlearn: 588.0827145\ttotal: 43.1s\tremaining: 24.9s\n",
      "634:\tlearn: 587.5877410\ttotal: 43.2s\tremaining: 24.8s\n",
      "635:\tlearn: 587.5829874\ttotal: 43.2s\tremaining: 24.8s\n",
      "636:\tlearn: 587.5793196\ttotal: 43.3s\tremaining: 24.7s\n",
      "637:\tlearn: 587.0324844\ttotal: 43.4s\tremaining: 24.6s\n",
      "638:\tlearn: 587.0324827\ttotal: 43.4s\tremaining: 24.5s\n",
      "639:\tlearn: 587.0322102\ttotal: 43.4s\tremaining: 24.4s\n",
      "640:\tlearn: 586.1665751\ttotal: 43.5s\tremaining: 24.4s\n",
      "641:\tlearn: 586.1660739\ttotal: 43.6s\tremaining: 24.3s\n",
      "642:\tlearn: 586.1092664\ttotal: 43.7s\tremaining: 24.3s\n",
      "643:\tlearn: 585.8727539\ttotal: 43.7s\tremaining: 24.2s\n",
      "644:\tlearn: 585.8719682\ttotal: 43.9s\tremaining: 24.1s\n",
      "645:\tlearn: 585.8719677\ttotal: 43.9s\tremaining: 24s\n",
      "646:\tlearn: 585.8684923\ttotal: 43.9s\tremaining: 24s\n",
      "647:\tlearn: 585.7920102\ttotal: 44s\tremaining: 23.9s\n",
      "648:\tlearn: 585.7351817\ttotal: 44s\tremaining: 23.8s\n",
      "649:\tlearn: 584.7731905\ttotal: 44.1s\tremaining: 23.8s\n",
      "650:\tlearn: 584.6830309\ttotal: 44.2s\tremaining: 23.7s\n",
      "651:\tlearn: 584.6791120\ttotal: 44.3s\tremaining: 23.6s\n",
      "652:\tlearn: 584.6756932\ttotal: 44.4s\tremaining: 23.6s\n",
      "653:\tlearn: 584.6639785\ttotal: 44.5s\tremaining: 23.5s\n",
      "654:\tlearn: 584.6609812\ttotal: 44.5s\tremaining: 23.4s\n",
      "655:\tlearn: 584.1556249\ttotal: 44.6s\tremaining: 23.4s\n",
      "656:\tlearn: 584.1555767\ttotal: 44.6s\tremaining: 23.3s\n",
      "657:\tlearn: 584.1553961\ttotal: 44.6s\tremaining: 23.2s\n",
      "658:\tlearn: 584.0968263\ttotal: 44.8s\tremaining: 23.2s\n",
      "659:\tlearn: 584.0722483\ttotal: 44.8s\tremaining: 23.1s\n",
      "660:\tlearn: 584.0693861\ttotal: 44.9s\tremaining: 23s\n",
      "661:\tlearn: 584.0693453\ttotal: 44.9s\tremaining: 22.9s\n",
      "662:\tlearn: 584.0270106\ttotal: 45s\tremaining: 22.9s\n",
      "663:\tlearn: 584.0244451\ttotal: 45s\tremaining: 22.8s\n",
      "664:\tlearn: 583.7740593\ttotal: 45.1s\tremaining: 22.7s\n",
      "665:\tlearn: 583.7740517\ttotal: 45.2s\tremaining: 22.7s\n",
      "666:\tlearn: 583.7486718\ttotal: 45.2s\tremaining: 22.6s\n",
      "667:\tlearn: 583.7246847\ttotal: 45.3s\tremaining: 22.5s\n",
      "668:\tlearn: 583.7232117\ttotal: 45.4s\tremaining: 22.4s\n",
      "669:\tlearn: 583.7229362\ttotal: 45.4s\tremaining: 22.4s\n",
      "670:\tlearn: 583.6953224\ttotal: 45.4s\tremaining: 22.3s\n",
      "671:\tlearn: 583.6887916\ttotal: 45.5s\tremaining: 22.2s\n",
      "672:\tlearn: 583.6867465\ttotal: 45.5s\tremaining: 22.1s\n",
      "673:\tlearn: 583.6867315\ttotal: 45.6s\tremaining: 22s\n",
      "674:\tlearn: 583.6867276\ttotal: 45.6s\tremaining: 21.9s\n",
      "675:\tlearn: 583.6865772\ttotal: 45.6s\tremaining: 21.9s\n",
      "676:\tlearn: 583.6310521\ttotal: 45.7s\tremaining: 21.8s\n",
      "677:\tlearn: 583.4742034\ttotal: 45.8s\tremaining: 21.7s\n",
      "678:\tlearn: 583.4615429\ttotal: 45.9s\tremaining: 21.7s\n",
      "679:\tlearn: 583.4498481\ttotal: 46s\tremaining: 21.6s\n",
      "680:\tlearn: 583.4456017\ttotal: 46s\tremaining: 21.5s\n",
      "681:\tlearn: 583.4443760\ttotal: 46s\tremaining: 21.5s\n",
      "682:\tlearn: 583.4443677\ttotal: 46s\tremaining: 21.4s\n",
      "683:\tlearn: 582.7353727\ttotal: 46.2s\tremaining: 21.3s\n",
      "684:\tlearn: 582.0901295\ttotal: 46.3s\tremaining: 21.3s\n",
      "685:\tlearn: 582.0901013\ttotal: 46.3s\tremaining: 21.2s\n",
      "686:\tlearn: 582.0540243\ttotal: 46.4s\tremaining: 21.2s\n",
      "687:\tlearn: 582.0537559\ttotal: 46.5s\tremaining: 21.1s\n",
      "688:\tlearn: 581.9993941\ttotal: 46.6s\tremaining: 21s\n",
      "689:\tlearn: 581.9852937\ttotal: 46.6s\tremaining: 21s\n",
      "690:\tlearn: 581.9487327\ttotal: 46.8s\tremaining: 20.9s\n",
      "691:\tlearn: 581.8576885\ttotal: 46.8s\tremaining: 20.8s\n",
      "692:\tlearn: 581.8576532\ttotal: 46.8s\tremaining: 20.7s\n",
      "693:\tlearn: 581.8568896\ttotal: 46.8s\tremaining: 20.7s\n",
      "694:\tlearn: 581.8565748\ttotal: 46.9s\tremaining: 20.6s\n",
      "695:\tlearn: 581.8541056\ttotal: 46.9s\tremaining: 20.5s\n",
      "696:\tlearn: 581.8502004\ttotal: 47s\tremaining: 20.4s\n",
      "697:\tlearn: 581.8409673\ttotal: 47.1s\tremaining: 20.4s\n",
      "698:\tlearn: 581.8340999\ttotal: 47.1s\tremaining: 20.3s\n",
      "699:\tlearn: 581.8327167\ttotal: 47.2s\tremaining: 20.2s\n",
      "700:\tlearn: 581.8324345\ttotal: 47.3s\tremaining: 20.2s\n",
      "701:\tlearn: 581.8320191\ttotal: 47.3s\tremaining: 20.1s\n",
      "702:\tlearn: 581.6289626\ttotal: 47.4s\tremaining: 20s\n",
      "703:\tlearn: 581.6237317\ttotal: 47.5s\tremaining: 20s\n",
      "704:\tlearn: 581.6230124\ttotal: 47.5s\tremaining: 19.9s\n",
      "705:\tlearn: 581.3882557\ttotal: 47.7s\tremaining: 19.9s\n",
      "706:\tlearn: 581.3251984\ttotal: 47.7s\tremaining: 19.8s\n",
      "707:\tlearn: 581.3224939\ttotal: 47.7s\tremaining: 19.7s\n",
      "708:\tlearn: 581.3155886\ttotal: 47.9s\tremaining: 19.6s\n",
      "709:\tlearn: 581.3137347\ttotal: 47.9s\tremaining: 19.6s\n",
      "710:\tlearn: 581.3122151\ttotal: 47.9s\tremaining: 19.5s\n",
      "711:\tlearn: 581.0886956\ttotal: 48s\tremaining: 19.4s\n",
      "712:\tlearn: 581.0665823\ttotal: 48.2s\tremaining: 19.4s\n",
      "713:\tlearn: 581.0616570\ttotal: 48.2s\tremaining: 19.3s\n",
      "714:\tlearn: 581.0148700\ttotal: 48.3s\tremaining: 19.2s\n",
      "715:\tlearn: 581.0137997\ttotal: 48.3s\tremaining: 19.2s\n",
      "716:\tlearn: 581.0137989\ttotal: 48.3s\tremaining: 19.1s\n",
      "717:\tlearn: 580.8808829\ttotal: 48.4s\tremaining: 19s\n",
      "718:\tlearn: 580.8751934\ttotal: 48.4s\tremaining: 18.9s\n",
      "719:\tlearn: 580.8751859\ttotal: 48.4s\tremaining: 18.8s\n",
      "720:\tlearn: 580.8746729\ttotal: 48.4s\tremaining: 18.7s\n",
      "721:\tlearn: 580.8358555\ttotal: 48.6s\tremaining: 18.7s\n",
      "722:\tlearn: 580.6143745\ttotal: 48.7s\tremaining: 18.7s\n",
      "723:\tlearn: 580.6132262\ttotal: 48.7s\tremaining: 18.6s\n",
      "724:\tlearn: 580.6132251\ttotal: 48.7s\tremaining: 18.5s\n",
      "725:\tlearn: 580.4642060\ttotal: 48.9s\tremaining: 18.4s\n",
      "726:\tlearn: 580.4616555\ttotal: 48.9s\tremaining: 18.4s\n",
      "727:\tlearn: 580.4248893\ttotal: 49s\tremaining: 18.3s\n",
      "728:\tlearn: 580.4214816\ttotal: 49s\tremaining: 18.2s\n",
      "729:\tlearn: 580.4177174\ttotal: 49.2s\tremaining: 18.2s\n",
      "730:\tlearn: 580.4177151\ttotal: 49.2s\tremaining: 18.1s\n",
      "731:\tlearn: 580.4157228\ttotal: 49.2s\tremaining: 18s\n",
      "732:\tlearn: 580.4156922\ttotal: 49.2s\tremaining: 17.9s\n",
      "733:\tlearn: 580.4153226\ttotal: 49.2s\tremaining: 17.8s\n",
      "734:\tlearn: 580.4153145\ttotal: 49.3s\tremaining: 17.8s\n",
      "735:\tlearn: 580.4153050\ttotal: 49.3s\tremaining: 17.7s\n",
      "736:\tlearn: 580.4077882\ttotal: 49.3s\tremaining: 17.6s\n",
      "737:\tlearn: 580.4077857\ttotal: 49.4s\tremaining: 17.5s\n",
      "738:\tlearn: 580.3285844\ttotal: 49.5s\tremaining: 17.5s\n",
      "739:\tlearn: 580.3238316\ttotal: 49.6s\tremaining: 17.4s\n",
      "740:\tlearn: 580.3238293\ttotal: 49.6s\tremaining: 17.3s\n",
      "741:\tlearn: 580.3233166\ttotal: 49.6s\tremaining: 17.3s\n",
      "742:\tlearn: 580.3181067\ttotal: 49.7s\tremaining: 17.2s\n",
      "743:\tlearn: 580.3178004\ttotal: 49.7s\tremaining: 17.1s\n",
      "744:\tlearn: 580.3178001\ttotal: 49.7s\tremaining: 17s\n",
      "745:\tlearn: 580.3165418\ttotal: 49.8s\tremaining: 17s\n",
      "746:\tlearn: 580.3162679\ttotal: 49.9s\tremaining: 16.9s\n",
      "747:\tlearn: 580.3130548\ttotal: 49.9s\tremaining: 16.8s\n",
      "748:\tlearn: 580.3121286\ttotal: 49.9s\tremaining: 16.7s\n",
      "749:\tlearn: 580.3105968\ttotal: 49.9s\tremaining: 16.6s\n",
      "750:\tlearn: 580.3105957\ttotal: 50s\tremaining: 16.6s\n",
      "751:\tlearn: 580.2842581\ttotal: 50.1s\tremaining: 16.5s\n",
      "752:\tlearn: 580.2711396\ttotal: 50.2s\tremaining: 16.5s\n",
      "753:\tlearn: 580.2164478\ttotal: 50.3s\tremaining: 16.4s\n",
      "754:\tlearn: 580.0648625\ttotal: 50.4s\tremaining: 16.4s\n",
      "755:\tlearn: 579.9778102\ttotal: 50.6s\tremaining: 16.3s\n",
      "756:\tlearn: 579.9777832\ttotal: 50.6s\tremaining: 16.2s\n",
      "757:\tlearn: 579.9771198\ttotal: 50.6s\tremaining: 16.2s\n",
      "758:\tlearn: 579.9770019\ttotal: 50.6s\tremaining: 16.1s\n",
      "759:\tlearn: 579.9715094\ttotal: 50.6s\tremaining: 16s\n",
      "760:\tlearn: 579.9695751\ttotal: 50.7s\tremaining: 15.9s\n",
      "761:\tlearn: 579.9658370\ttotal: 50.8s\tremaining: 15.9s\n",
      "762:\tlearn: 579.9635538\ttotal: 50.9s\tremaining: 15.8s\n",
      "763:\tlearn: 579.8834111\ttotal: 51s\tremaining: 15.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "764:\tlearn: 579.8833235\ttotal: 51s\tremaining: 15.7s\n",
      "765:\tlearn: 579.4857269\ttotal: 51.2s\tremaining: 15.6s\n",
      "766:\tlearn: 579.4740431\ttotal: 51.2s\tremaining: 15.6s\n",
      "767:\tlearn: 579.3857392\ttotal: 51.3s\tremaining: 15.5s\n",
      "768:\tlearn: 579.2276814\ttotal: 51.4s\tremaining: 15.4s\n",
      "769:\tlearn: 579.1841297\ttotal: 51.5s\tremaining: 15.4s\n",
      "770:\tlearn: 579.1840658\ttotal: 51.5s\tremaining: 15.3s\n",
      "771:\tlearn: 579.0730279\ttotal: 51.7s\tremaining: 15.3s\n",
      "772:\tlearn: 579.0730151\ttotal: 51.7s\tremaining: 15.2s\n",
      "773:\tlearn: 579.0730047\ttotal: 51.7s\tremaining: 15.1s\n",
      "774:\tlearn: 579.0724276\ttotal: 51.7s\tremaining: 15s\n",
      "775:\tlearn: 579.0724273\ttotal: 51.7s\tremaining: 14.9s\n",
      "776:\tlearn: 579.0354814\ttotal: 51.8s\tremaining: 14.9s\n",
      "777:\tlearn: 579.0158795\ttotal: 51.8s\tremaining: 14.8s\n",
      "778:\tlearn: 579.0157504\ttotal: 51.9s\tremaining: 14.7s\n",
      "779:\tlearn: 579.0140636\ttotal: 51.9s\tremaining: 14.6s\n",
      "780:\tlearn: 579.0140117\ttotal: 52s\tremaining: 14.6s\n",
      "781:\tlearn: 579.0138155\ttotal: 52.1s\tremaining: 14.5s\n",
      "782:\tlearn: 579.0005596\ttotal: 52.2s\tremaining: 14.5s\n",
      "783:\tlearn: 578.9956308\ttotal: 52.3s\tremaining: 14.4s\n",
      "784:\tlearn: 578.9917332\ttotal: 52.3s\tremaining: 14.3s\n",
      "785:\tlearn: 578.9917326\ttotal: 52.3s\tremaining: 14.2s\n",
      "786:\tlearn: 578.9888266\ttotal: 52.4s\tremaining: 14.2s\n",
      "787:\tlearn: 578.9888054\ttotal: 52.4s\tremaining: 14.1s\n",
      "788:\tlearn: 578.9806033\ttotal: 52.4s\tremaining: 14s\n",
      "789:\tlearn: 578.7251280\ttotal: 52.6s\tremaining: 14s\n",
      "790:\tlearn: 578.7244390\ttotal: 52.6s\tremaining: 13.9s\n",
      "791:\tlearn: 578.7244387\ttotal: 52.6s\tremaining: 13.8s\n",
      "792:\tlearn: 578.6859106\ttotal: 52.7s\tremaining: 13.8s\n",
      "793:\tlearn: 578.6858513\ttotal: 52.7s\tremaining: 13.7s\n",
      "794:\tlearn: 578.6739959\ttotal: 52.9s\tremaining: 13.6s\n",
      "795:\tlearn: 578.3665634\ttotal: 53s\tremaining: 13.6s\n",
      "796:\tlearn: 578.3627940\ttotal: 53.1s\tremaining: 13.5s\n",
      "797:\tlearn: 578.3625519\ttotal: 53.1s\tremaining: 13.4s\n",
      "798:\tlearn: 578.3591636\ttotal: 53.2s\tremaining: 13.4s\n",
      "799:\tlearn: 578.3591100\ttotal: 53.2s\tremaining: 13.3s\n",
      "800:\tlearn: 578.3436423\ttotal: 53.2s\tremaining: 13.2s\n",
      "801:\tlearn: 578.3337931\ttotal: 53.3s\tremaining: 13.2s\n",
      "802:\tlearn: 578.3313408\ttotal: 53.3s\tremaining: 13.1s\n",
      "803:\tlearn: 578.2897817\ttotal: 53.3s\tremaining: 13s\n",
      "804:\tlearn: 578.2861579\ttotal: 53.5s\tremaining: 12.9s\n",
      "805:\tlearn: 577.4589511\ttotal: 53.6s\tremaining: 12.9s\n",
      "806:\tlearn: 577.4589348\ttotal: 53.6s\tremaining: 12.8s\n",
      "807:\tlearn: 577.4587647\ttotal: 53.6s\tremaining: 12.7s\n",
      "808:\tlearn: 577.3691570\ttotal: 53.7s\tremaining: 12.7s\n",
      "809:\tlearn: 577.3690906\ttotal: 53.7s\tremaining: 12.6s\n",
      "810:\tlearn: 577.3680177\ttotal: 53.9s\tremaining: 12.6s\n",
      "811:\tlearn: 577.3680176\ttotal: 53.9s\tremaining: 12.5s\n",
      "812:\tlearn: 577.3658968\ttotal: 53.9s\tremaining: 12.4s\n",
      "813:\tlearn: 577.3658785\ttotal: 53.9s\tremaining: 12.3s\n",
      "814:\tlearn: 577.3658407\ttotal: 53.9s\tremaining: 12.2s\n",
      "815:\tlearn: 577.3402922\ttotal: 54.1s\tremaining: 12.2s\n",
      "816:\tlearn: 577.3398701\ttotal: 54.2s\tremaining: 12.1s\n",
      "817:\tlearn: 577.3398618\ttotal: 54.2s\tremaining: 12.1s\n",
      "818:\tlearn: 577.2883361\ttotal: 54.3s\tremaining: 12s\n",
      "819:\tlearn: 577.2853962\ttotal: 54.4s\tremaining: 12s\n",
      "820:\tlearn: 577.2853657\ttotal: 54.5s\tremaining: 11.9s\n",
      "821:\tlearn: 577.2839084\ttotal: 54.5s\tremaining: 11.8s\n",
      "822:\tlearn: 577.2368187\ttotal: 54.6s\tremaining: 11.8s\n",
      "823:\tlearn: 577.0242544\ttotal: 54.8s\tremaining: 11.7s\n",
      "824:\tlearn: 577.0217372\ttotal: 54.8s\tremaining: 11.6s\n",
      "825:\tlearn: 577.0214373\ttotal: 54.8s\tremaining: 11.5s\n",
      "826:\tlearn: 577.0213387\ttotal: 54.9s\tremaining: 11.5s\n",
      "827:\tlearn: 576.9515086\ttotal: 54.9s\tremaining: 11.4s\n",
      "828:\tlearn: 576.9515062\ttotal: 54.9s\tremaining: 11.3s\n",
      "829:\tlearn: 576.9505907\ttotal: 55s\tremaining: 11.3s\n",
      "830:\tlearn: 576.9455627\ttotal: 55.1s\tremaining: 11.2s\n",
      "831:\tlearn: 576.9453986\ttotal: 55.1s\tremaining: 11.1s\n",
      "832:\tlearn: 576.9452727\ttotal: 55.2s\tremaining: 11.1s\n",
      "833:\tlearn: 576.9449772\ttotal: 55.3s\tremaining: 11s\n",
      "834:\tlearn: 576.9448126\ttotal: 55.3s\tremaining: 10.9s\n",
      "835:\tlearn: 576.9436972\ttotal: 55.3s\tremaining: 10.9s\n",
      "836:\tlearn: 576.9390428\ttotal: 55.4s\tremaining: 10.8s\n",
      "837:\tlearn: 576.9389621\ttotal: 55.5s\tremaining: 10.7s\n",
      "838:\tlearn: 576.8038150\ttotal: 55.5s\tremaining: 10.7s\n",
      "839:\tlearn: 576.8027873\ttotal: 55.6s\tremaining: 10.6s\n",
      "840:\tlearn: 576.8019323\ttotal: 55.6s\tremaining: 10.5s\n",
      "841:\tlearn: 576.7768834\ttotal: 55.7s\tremaining: 10.5s\n",
      "842:\tlearn: 576.7754239\ttotal: 55.8s\tremaining: 10.4s\n",
      "843:\tlearn: 576.7752164\ttotal: 55.9s\tremaining: 10.3s\n",
      "844:\tlearn: 576.7746632\ttotal: 55.9s\tremaining: 10.3s\n",
      "845:\tlearn: 576.7743449\ttotal: 55.9s\tremaining: 10.2s\n",
      "846:\tlearn: 576.7743443\ttotal: 55.9s\tremaining: 10.1s\n",
      "847:\tlearn: 576.3680092\ttotal: 56.1s\tremaining: 10.1s\n",
      "848:\tlearn: 576.3677409\ttotal: 56.1s\tremaining: 9.98s\n",
      "849:\tlearn: 576.3677396\ttotal: 56.1s\tremaining: 9.9s\n",
      "850:\tlearn: 576.3645355\ttotal: 56.2s\tremaining: 9.83s\n",
      "851:\tlearn: 576.3641644\ttotal: 56.2s\tremaining: 9.77s\n",
      "852:\tlearn: 576.3641320\ttotal: 56.3s\tremaining: 9.7s\n",
      "853:\tlearn: 576.3641279\ttotal: 56.3s\tremaining: 9.62s\n",
      "854:\tlearn: 576.3570190\ttotal: 56.3s\tremaining: 9.55s\n",
      "855:\tlearn: 576.3104018\ttotal: 56.5s\tremaining: 9.5s\n",
      "856:\tlearn: 576.3038544\ttotal: 56.6s\tremaining: 9.44s\n",
      "857:\tlearn: 576.1614313\ttotal: 56.7s\tremaining: 9.38s\n",
      "858:\tlearn: 576.1614263\ttotal: 56.7s\tremaining: 9.31s\n",
      "859:\tlearn: 576.1403463\ttotal: 56.8s\tremaining: 9.24s\n",
      "860:\tlearn: 576.1403454\ttotal: 56.8s\tremaining: 9.16s\n",
      "861:\tlearn: 576.1403393\ttotal: 56.8s\tremaining: 9.09s\n",
      "862:\tlearn: 576.0719024\ttotal: 56.8s\tremaining: 9.02s\n",
      "863:\tlearn: 576.0679228\ttotal: 56.9s\tremaining: 8.95s\n",
      "864:\tlearn: 576.0679225\ttotal: 56.9s\tremaining: 8.88s\n",
      "865:\tlearn: 576.0678972\ttotal: 56.9s\tremaining: 8.81s\n",
      "866:\tlearn: 576.0673667\ttotal: 56.9s\tremaining: 8.73s\n",
      "867:\tlearn: 575.9664488\ttotal: 57.1s\tremaining: 8.68s\n",
      "868:\tlearn: 575.9591781\ttotal: 57.1s\tremaining: 8.61s\n",
      "869:\tlearn: 575.2554740\ttotal: 57.2s\tremaining: 8.55s\n",
      "870:\tlearn: 575.2409821\ttotal: 57.3s\tremaining: 8.48s\n",
      "871:\tlearn: 575.2396638\ttotal: 57.4s\tremaining: 8.42s\n",
      "872:\tlearn: 575.2279747\ttotal: 57.5s\tremaining: 8.36s\n",
      "873:\tlearn: 575.2268567\ttotal: 57.5s\tremaining: 8.29s\n",
      "874:\tlearn: 575.2231344\ttotal: 57.5s\tremaining: 8.22s\n",
      "875:\tlearn: 575.0204561\ttotal: 57.7s\tremaining: 8.16s\n",
      "876:\tlearn: 575.0131610\ttotal: 57.7s\tremaining: 8.09s\n",
      "877:\tlearn: 575.0112772\ttotal: 57.8s\tremaining: 8.03s\n",
      "878:\tlearn: 575.0008492\ttotal: 57.9s\tremaining: 7.97s\n",
      "879:\tlearn: 575.0005619\ttotal: 57.9s\tremaining: 7.9s\n",
      "880:\tlearn: 575.0002517\ttotal: 58s\tremaining: 7.83s\n",
      "881:\tlearn: 574.9913436\ttotal: 58.1s\tremaining: 7.77s\n",
      "882:\tlearn: 574.9900093\ttotal: 58.1s\tremaining: 7.7s\n",
      "883:\tlearn: 574.9814742\ttotal: 58.2s\tremaining: 7.64s\n",
      "884:\tlearn: 574.9788699\ttotal: 58.2s\tremaining: 7.56s\n",
      "885:\tlearn: 574.9784852\ttotal: 58.3s\tremaining: 7.5s\n",
      "886:\tlearn: 574.9782314\ttotal: 58.3s\tremaining: 7.42s\n",
      "887:\tlearn: 574.9766390\ttotal: 58.3s\tremaining: 7.35s\n",
      "888:\tlearn: 574.9582470\ttotal: 58.3s\tremaining: 7.28s\n",
      "889:\tlearn: 574.9287234\ttotal: 58.4s\tremaining: 7.21s\n",
      "890:\tlearn: 574.8030794\ttotal: 58.5s\tremaining: 7.16s\n",
      "891:\tlearn: 573.9380341\ttotal: 58.6s\tremaining: 7.1s\n",
      "892:\tlearn: 573.9359441\ttotal: 58.7s\tremaining: 7.04s\n",
      "893:\tlearn: 573.9321808\ttotal: 58.9s\tremaining: 6.98s\n",
      "894:\tlearn: 573.8714229\ttotal: 58.9s\tremaining: 6.91s\n",
      "895:\tlearn: 573.8396759\ttotal: 58.9s\tremaining: 6.84s\n",
      "896:\tlearn: 573.1872140\ttotal: 59.1s\tremaining: 6.78s\n",
      "897:\tlearn: 573.1870796\ttotal: 59.1s\tremaining: 6.71s\n",
      "898:\tlearn: 572.8852080\ttotal: 59.2s\tremaining: 6.65s\n",
      "899:\tlearn: 572.8618854\ttotal: 59.3s\tremaining: 6.59s\n",
      "900:\tlearn: 572.8569743\ttotal: 59.4s\tremaining: 6.53s\n",
      "901:\tlearn: 572.8010511\ttotal: 59.5s\tremaining: 6.47s\n",
      "902:\tlearn: 572.7496191\ttotal: 59.6s\tremaining: 6.41s\n",
      "903:\tlearn: 572.7484140\ttotal: 59.7s\tremaining: 6.33s\n",
      "904:\tlearn: 572.7373761\ttotal: 59.8s\tremaining: 6.28s\n",
      "905:\tlearn: 572.7316219\ttotal: 59.8s\tremaining: 6.2s\n",
      "906:\tlearn: 572.7294858\ttotal: 59.9s\tremaining: 6.14s\n",
      "907:\tlearn: 572.7259367\ttotal: 1m\tremaining: 6.08s\n",
      "908:\tlearn: 572.0660948\ttotal: 1m\tremaining: 6.02s\n",
      "909:\tlearn: 572.0242641\ttotal: 1m\tremaining: 5.96s\n",
      "910:\tlearn: 571.9138913\ttotal: 1m\tremaining: 5.9s\n",
      "911:\tlearn: 571.9138453\ttotal: 1m\tremaining: 5.83s\n",
      "912:\tlearn: 571.9106674\ttotal: 1m\tremaining: 5.76s\n",
      "913:\tlearn: 571.9106674\ttotal: 1m\tremaining: 5.68s\n",
      "914:\tlearn: 571.8207534\ttotal: 1m\tremaining: 5.62s\n",
      "915:\tlearn: 571.8125276\ttotal: 1m\tremaining: 5.56s\n",
      "916:\tlearn: 571.8058807\ttotal: 1m\tremaining: 5.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917:\tlearn: 570.9676647\ttotal: 1m\tremaining: 5.43s\n",
      "918:\tlearn: 570.9054031\ttotal: 1m\tremaining: 5.37s\n",
      "919:\tlearn: 570.8974903\ttotal: 1m\tremaining: 5.3s\n",
      "920:\tlearn: 570.8807883\ttotal: 1m 1s\tremaining: 5.23s\n",
      "921:\tlearn: 570.8537472\ttotal: 1m 1s\tremaining: 5.17s\n",
      "922:\tlearn: 570.8456610\ttotal: 1m 1s\tremaining: 5.1s\n",
      "923:\tlearn: 570.8450635\ttotal: 1m 1s\tremaining: 5.04s\n",
      "924:\tlearn: 570.8450603\ttotal: 1m 1s\tremaining: 4.97s\n",
      "925:\tlearn: 570.6099325\ttotal: 1m 1s\tremaining: 4.9s\n",
      "926:\tlearn: 570.6092249\ttotal: 1m 1s\tremaining: 4.83s\n",
      "927:\tlearn: 570.6092242\ttotal: 1m 1s\tremaining: 4.76s\n",
      "928:\tlearn: 570.6074229\ttotal: 1m 1s\tremaining: 4.69s\n",
      "929:\tlearn: 570.6036056\ttotal: 1m 1s\tremaining: 4.63s\n",
      "930:\tlearn: 570.6013161\ttotal: 1m 1s\tremaining: 4.56s\n",
      "931:\tlearn: 570.6013139\ttotal: 1m 1s\tremaining: 4.49s\n",
      "932:\tlearn: 570.5635950\ttotal: 1m 1s\tremaining: 4.42s\n",
      "933:\tlearn: 570.5633715\ttotal: 1m 1s\tremaining: 4.36s\n",
      "934:\tlearn: 570.2980977\ttotal: 1m 1s\tremaining: 4.3s\n",
      "935:\tlearn: 570.2979522\ttotal: 1m 1s\tremaining: 4.23s\n",
      "936:\tlearn: 570.2955721\ttotal: 1m 1s\tremaining: 4.16s\n",
      "937:\tlearn: 570.2955648\ttotal: 1m 1s\tremaining: 4.09s\n",
      "938:\tlearn: 570.2945297\ttotal: 1m 1s\tremaining: 4.02s\n",
      "939:\tlearn: 570.2605474\ttotal: 1m 1s\tremaining: 3.96s\n",
      "940:\tlearn: 570.2358241\ttotal: 1m 2s\tremaining: 3.89s\n",
      "941:\tlearn: 570.2343383\ttotal: 1m 2s\tremaining: 3.83s\n",
      "942:\tlearn: 570.2331151\ttotal: 1m 2s\tremaining: 3.76s\n",
      "943:\tlearn: 570.0584213\ttotal: 1m 2s\tremaining: 3.7s\n",
      "944:\tlearn: 570.0580528\ttotal: 1m 2s\tremaining: 3.64s\n",
      "945:\tlearn: 569.7960783\ttotal: 1m 2s\tremaining: 3.57s\n",
      "946:\tlearn: 569.4280059\ttotal: 1m 2s\tremaining: 3.51s\n",
      "947:\tlearn: 569.4239997\ttotal: 1m 2s\tremaining: 3.44s\n",
      "948:\tlearn: 569.4233349\ttotal: 1m 2s\tremaining: 3.37s\n",
      "949:\tlearn: 569.4174031\ttotal: 1m 2s\tremaining: 3.31s\n",
      "950:\tlearn: 569.4174022\ttotal: 1m 2s\tremaining: 3.24s\n",
      "951:\tlearn: 569.4171261\ttotal: 1m 2s\tremaining: 3.17s\n",
      "952:\tlearn: 569.3812823\ttotal: 1m 2s\tremaining: 3.11s\n",
      "953:\tlearn: 569.3797675\ttotal: 1m 3s\tremaining: 3.04s\n",
      "954:\tlearn: 569.3797370\ttotal: 1m 3s\tremaining: 2.97s\n",
      "955:\tlearn: 569.3797367\ttotal: 1m 3s\tremaining: 2.9s\n",
      "956:\tlearn: 569.3797175\ttotal: 1m 3s\tremaining: 2.83s\n",
      "957:\tlearn: 569.3791241\ttotal: 1m 3s\tremaining: 2.77s\n",
      "958:\tlearn: 569.3782401\ttotal: 1m 3s\tremaining: 2.7s\n",
      "959:\tlearn: 569.3631188\ttotal: 1m 3s\tremaining: 2.63s\n",
      "960:\tlearn: 569.3340065\ttotal: 1m 3s\tremaining: 2.57s\n",
      "961:\tlearn: 569.3318546\ttotal: 1m 3s\tremaining: 2.5s\n",
      "962:\tlearn: 569.3309139\ttotal: 1m 3s\tremaining: 2.44s\n",
      "963:\tlearn: 569.3309138\ttotal: 1m 3s\tremaining: 2.37s\n",
      "964:\tlearn: 569.3309137\ttotal: 1m 3s\tremaining: 2.3s\n",
      "965:\tlearn: 569.3307116\ttotal: 1m 3s\tremaining: 2.24s\n",
      "966:\tlearn: 569.3298367\ttotal: 1m 3s\tremaining: 2.17s\n",
      "967:\tlearn: 569.0107173\ttotal: 1m 3s\tremaining: 2.11s\n",
      "968:\tlearn: 569.0055461\ttotal: 1m 3s\tremaining: 2.04s\n",
      "969:\tlearn: 569.0036785\ttotal: 1m 3s\tremaining: 1.98s\n",
      "970:\tlearn: 569.0031119\ttotal: 1m 4s\tremaining: 1.91s\n",
      "971:\tlearn: 569.0031110\ttotal: 1m 4s\tremaining: 1.84s\n",
      "972:\tlearn: 568.7322511\ttotal: 1m 4s\tremaining: 1.78s\n",
      "973:\tlearn: 568.7284950\ttotal: 1m 4s\tremaining: 1.72s\n",
      "974:\tlearn: 568.7226825\ttotal: 1m 4s\tremaining: 1.65s\n",
      "975:\tlearn: 568.7082603\ttotal: 1m 4s\tremaining: 1.58s\n",
      "976:\tlearn: 568.7081869\ttotal: 1m 4s\tremaining: 1.52s\n",
      "977:\tlearn: 568.7072130\ttotal: 1m 4s\tremaining: 1.45s\n",
      "978:\tlearn: 568.7047997\ttotal: 1m 4s\tremaining: 1.38s\n",
      "979:\tlearn: 568.7046451\ttotal: 1m 4s\tremaining: 1.32s\n",
      "980:\tlearn: 568.7043746\ttotal: 1m 4s\tremaining: 1.25s\n",
      "981:\tlearn: 568.7029482\ttotal: 1m 4s\tremaining: 1.19s\n",
      "982:\tlearn: 568.7029473\ttotal: 1m 4s\tremaining: 1.12s\n",
      "983:\tlearn: 568.6618175\ttotal: 1m 4s\tremaining: 1.05s\n",
      "984:\tlearn: 568.6617019\ttotal: 1m 4s\tremaining: 989ms\n",
      "985:\tlearn: 568.6616973\ttotal: 1m 4s\tremaining: 923ms\n",
      "986:\tlearn: 568.6320816\ttotal: 1m 5s\tremaining: 857ms\n",
      "987:\tlearn: 568.6309887\ttotal: 1m 5s\tremaining: 790ms\n",
      "988:\tlearn: 568.6281750\ttotal: 1m 5s\tremaining: 725ms\n",
      "989:\tlearn: 568.6281744\ttotal: 1m 5s\tremaining: 659ms\n",
      "990:\tlearn: 568.6009012\ttotal: 1m 5s\tremaining: 593ms\n",
      "991:\tlearn: 568.5948009\ttotal: 1m 5s\tremaining: 527ms\n",
      "992:\tlearn: 568.5921152\ttotal: 1m 5s\tremaining: 462ms\n",
      "993:\tlearn: 568.5921147\ttotal: 1m 5s\tremaining: 396ms\n",
      "994:\tlearn: 568.5853318\ttotal: 1m 5s\tremaining: 330ms\n",
      "995:\tlearn: 568.4837790\ttotal: 1m 5s\tremaining: 264ms\n",
      "996:\tlearn: 568.4796794\ttotal: 1m 5s\tremaining: 198ms\n",
      "997:\tlearn: 568.2215662\ttotal: 1m 5s\tremaining: 132ms\n",
      "998:\tlearn: 568.2136268\ttotal: 1m 6s\tremaining: 66.1ms\n",
      "999:\tlearn: 568.1262781\ttotal: 1m 6s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=GroupKFold(n_splits=5), error_score='raise-deprecating',\n",
       "             estimator=<catboost.core.CatBoostRegressor object at 0x0000018F08607E48>,\n",
       "             iid='warn', n_jobs=-2,\n",
       "             param_grid={'depth': [2, 5, 10], 'iterations': [100, 500, 1000],\n",
       "                         'learning_rate': [0.001, 0.01, 0.1]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=make_scorer(mape, greater_is_better=False), verbose=2)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'depth':[2, 5,10],\n",
    "              'iterations':[100,500,1000],\n",
    "              'learning_rate':[0.001,0.01,0.1,]} \n",
    "              #'l2_leaf_reg':[3,1,5,10,100],\n",
    "              #'border_count':[32,5,10,20,50,100,200],\n",
    "              #'ctr_border_count':[50,5,10,20,100,200],\n",
    "              #'thread_count':4}\n",
    "model_cb = CatBoostRegressor()\n",
    "scorer = make_scorer(mape, greater_is_better = False)                       \n",
    "    \n",
    "grid_cb = GridSearchCV(model_cb, \n",
    "                        param_grid, \n",
    "                        scoring = scorer,\n",
    "                        n_jobs=-2,\n",
    "                        verbose=2,\n",
    "                        cv = GroupKFold(n_splits=5))\n",
    "                                \n",
    "grid_cb.fit(X_train_scaled, y_train, groups = groups)              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost mape: 77.90466221155232 {'depth': 10, 'iterations': 1000, 'learning_rate': 0.1}\n",
      "CatBoost rmse: 4338.744151048376\n"
     ]
    }
   ],
   "source": [
    "print(\"CatBoost mape:\", mape_by_month(y_test, grid_cb.predict(X_test_scaled)), grid_cb.best_params_)\n",
    "print(\"CatBoost rmse:\", rmse(y_test, grid_cb.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Cat var:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand</th>\n",
       "      <th>Year</th>\n",
       "      <th>Quarter</th>\n",
       "      <th>Month_No</th>\n",
       "      <th>Week_No</th>\n",
       "      <th>W_Nielsen</th>\n",
       "      <th>SKU_Customer</th>\n",
       "      <th>SKU_Customer_month</th>\n",
       "      <th>Brand_month</th>\n",
       "      <th>Brand_SKU_Customer</th>\n",
       "      <th>Brand_SKU_Customer_month</th>\n",
       "      <th>SKU_Customer_month_Week_No</th>\n",
       "      <th>Brand_month_Week_No</th>\n",
       "      <th>Brand_SKU_Customer_Week_No</th>\n",
       "      <th>Brand_SKU_Customer_month_Week_No</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-0.746046</td>\n",
       "      <td>-1.262361</td>\n",
       "      <td>-1.265143</td>\n",
       "      <td>-1.541062</td>\n",
       "      <td>-1.625597</td>\n",
       "      <td>-0.407639</td>\n",
       "      <td>-1.551966</td>\n",
       "      <td>0.443909</td>\n",
       "      <td>-0.768579</td>\n",
       "      <td>0.574047</td>\n",
       "      <td>0.443909</td>\n",
       "      <td>-0.156012</td>\n",
       "      <td>-1.645016</td>\n",
       "      <td>-0.156012</td>\n",
       "      <td>-0.156012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-0.746046</td>\n",
       "      <td>-1.262361</td>\n",
       "      <td>-1.265143</td>\n",
       "      <td>-1.541062</td>\n",
       "      <td>-1.557834</td>\n",
       "      <td>-0.407639</td>\n",
       "      <td>-1.551966</td>\n",
       "      <td>0.443909</td>\n",
       "      <td>-0.768579</td>\n",
       "      <td>0.574047</td>\n",
       "      <td>0.443909</td>\n",
       "      <td>0.548017</td>\n",
       "      <td>-0.317668</td>\n",
       "      <td>0.548017</td>\n",
       "      <td>0.548017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-0.746046</td>\n",
       "      <td>-1.262361</td>\n",
       "      <td>-1.265143</td>\n",
       "      <td>-1.541062</td>\n",
       "      <td>-1.490071</td>\n",
       "      <td>-0.407639</td>\n",
       "      <td>-1.551966</td>\n",
       "      <td>0.443909</td>\n",
       "      <td>-0.768579</td>\n",
       "      <td>0.574047</td>\n",
       "      <td>0.443909</td>\n",
       "      <td>0.698721</td>\n",
       "      <td>-0.691186</td>\n",
       "      <td>0.698721</td>\n",
       "      <td>0.698721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-0.746046</td>\n",
       "      <td>-1.262361</td>\n",
       "      <td>-1.265143</td>\n",
       "      <td>-1.541062</td>\n",
       "      <td>-1.422308</td>\n",
       "      <td>-0.407639</td>\n",
       "      <td>-1.551966</td>\n",
       "      <td>0.443909</td>\n",
       "      <td>-0.768579</td>\n",
       "      <td>0.574047</td>\n",
       "      <td>0.443909</td>\n",
       "      <td>0.591043</td>\n",
       "      <td>0.074554</td>\n",
       "      <td>0.591043</td>\n",
       "      <td>0.591043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.746046</td>\n",
       "      <td>-1.262361</td>\n",
       "      <td>-1.265143</td>\n",
       "      <td>-1.246196</td>\n",
       "      <td>-1.354545</td>\n",
       "      <td>-0.407639</td>\n",
       "      <td>-1.551966</td>\n",
       "      <td>0.763108</td>\n",
       "      <td>-0.540984</td>\n",
       "      <td>0.574047</td>\n",
       "      <td>0.763108</td>\n",
       "      <td>0.782545</td>\n",
       "      <td>-0.817606</td>\n",
       "      <td>0.782545</td>\n",
       "      <td>0.782545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Brand      Year   Quarter  Month_No   Week_No  W_Nielsen  SKU_Customer  \\\n",
       "0 -0.746046 -1.262361 -1.265143 -1.541062 -1.625597  -0.407639     -1.551966   \n",
       "1 -0.746046 -1.262361 -1.265143 -1.541062 -1.557834  -0.407639     -1.551966   \n",
       "2 -0.746046 -1.262361 -1.265143 -1.541062 -1.490071  -0.407639     -1.551966   \n",
       "3 -0.746046 -1.262361 -1.265143 -1.541062 -1.422308  -0.407639     -1.551966   \n",
       "4 -0.746046 -1.262361 -1.265143 -1.246196 -1.354545  -0.407639     -1.551966   \n",
       "\n",
       "   SKU_Customer_month  Brand_month  Brand_SKU_Customer  \\\n",
       "0            0.443909    -0.768579            0.574047   \n",
       "1            0.443909    -0.768579            0.574047   \n",
       "2            0.443909    -0.768579            0.574047   \n",
       "3            0.443909    -0.768579            0.574047   \n",
       "4            0.763108    -0.540984            0.574047   \n",
       "\n",
       "   Brand_SKU_Customer_month  SKU_Customer_month_Week_No  Brand_month_Week_No  \\\n",
       "0                  0.443909                   -0.156012            -1.645016   \n",
       "1                  0.443909                    0.548017            -0.317668   \n",
       "2                  0.443909                    0.698721            -0.691186   \n",
       "3                  0.443909                    0.591043             0.074554   \n",
       "4                  0.763108                    0.782545            -0.817606   \n",
       "\n",
       "   Brand_SKU_Customer_Week_No  Brand_SKU_Customer_month_Week_No  \n",
       "0                   -0.156012                         -0.156012  \n",
       "1                    0.548017                          0.548017  \n",
       "2                    0.698721                          0.698721  \n",
       "3                    0.591043                          0.591043  \n",
       "4                    0.782545                          0.782545  "
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.74604627, -1.26236077, -1.26514343, ..., -1.64501591,\n",
       "        -0.15601172, -0.15601172],\n",
       "       [-0.74604627, -1.26236077, -1.26514343, ..., -0.31766764,\n",
       "         0.54801697,  0.54801697],\n",
       "       [-0.74604627, -1.26236077, -1.26514343, ..., -0.69118637,\n",
       "         0.69872102,  0.69872102],\n",
       "       ...,\n",
       "       [ 1.72362415,  1.55938683,  0.55589636, ..., -0.65033981,\n",
       "         0.59148929,  0.59148929],\n",
       "       [ 1.72362415,  1.55938683,  0.55589636, ..., -0.63545829,\n",
       "         1.16220286,  1.16220286],\n",
       "       [ 1.72362415,  1.55938683,  0.55589636, ..., -0.16692725,\n",
       "         1.20389171,  1.20389171]])"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-2)]: Using backend LokyBackend with 11 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'depth':[2, 5,10],\n",
    "              'iterations':[100,500,1000],\n",
    "              'learning_rate':[0.001,0.01,0.1,]} \n",
    "              #'l2_leaf_reg':[3,1,5,10,100],\n",
    "              #'border_count':[32,5,10,20,50,100,200],\n",
    "              #'ctr_border_count':[50,5,10,20,100,200],\n",
    "              #'thread_count':4}\n",
    "                \n",
    "model_cb = CatBoostRegressor()\n",
    "scorer = make_scorer(mape, greater_is_better = False)                       \n",
    "    \n",
    "grid_cb2 = GridSearchCV(model_cb, \n",
    "                        param_grid, \n",
    "                        scoring = scorer,\n",
    "                        n_jobs=-2,\n",
    "                        verbose=2,\n",
    "                        cv = GroupKFold(n_splits=5))\n",
    "                                \n",
    "grid_cb2.fit(np.array(X_train_scaled), y_train, groups = groups, cat_features= [0, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CatBoost mape:\", mape_by_month(y_test, grid_cb2.predict(X_test_scaled)), grid_cb.best_params_)\n",
    "print(\"CatBoost rmse:\", rmse(y_test, grid_cb2.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tuning another parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'depth':[2, 5,10],\n",
    "              'iterations':[100,500,1000],\n",
    "              'learning_rate':[0.001,0.01,0.1,]} \n",
    "              #'l2_leaf_reg':[3,1,5,10,100],\n",
    "              #'border_count':[32,5,10,20,50,100,200],\n",
    "              #'ctr_border_count':[50,5,10,20,100,200],\n",
    "              #'thread_count':4}\n",
    "                \n",
    "model_cb = CatBoostRegressor()\n",
    "scorer = make_scorer(mape, greater_is_better = False)                       \n",
    "    \n",
    "grid_cb2 = GridSearchCV(model_cb, \n",
    "                        param_grid, \n",
    "                        scoring = scorer,\n",
    "                        n_jobs=-2,\n",
    "                        verbose=2,\n",
    "                        cv = GroupKFold(n_splits=5))\n",
    "                                \n",
    "grid_cb2.fit(X_train_scaled, y_train, groups = groups, cat_features=cat_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With new featchers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 77.4485951889016 {'learning_rate': 0.1, 'max_depth': 8, 'n_estimators': 170, 'num_leaves': 9}\n",
      "lightgbm rmse: 4296.019034609904\n"
     ]
    }
   ],
   "source": [
    "# 77.56\n",
    "\n",
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : list(range(100, 180,10)),\n",
    "    'num_leaves': list(range(7,10, 1)),\n",
    "    # 'min_data_in_leaf': [10, 20, 40, 60, 100],\n",
    "    'max_depth': [2, 4, 6, 7, 8],\n",
    "    'learning_rate': [0.01, .016, .02, .1, .2, .3]}\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    # 'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    # 'reg_lambda': np.linspace(0.1, 0.95, 10)\n",
    "\n",
    "    \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm_n1 = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   n_jobs = -2,\n",
    "                   cv = GroupKFold(n_splits=5))\n",
    "\n",
    "\n",
    "gbm_n1.fit(X_train_scaled, y_train, groups=groups)\n",
    "\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm_n1.predict(X_test_scaled)), gbm_n1.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm_n1.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lightgbm mape: 77.43852063761443 {'learning_rate': 0.1, 'max_depth': 8, 'min_data_in_leaf': 10, 'n_estimators': 170, 'num_leaves': 9, 'reg_alpha': 0.1, 'reg_lambda': 0.3833333333333333}\n",
      "lightgbm rmse: 4290.256226353061\n"
     ]
    }
   ],
   "source": [
    "# 77.56\n",
    "\n",
    "X_train_scaled, X_test_scaled = diff_scale('norm', X_train, X_test)\n",
    "param_grid = {\n",
    "    'n_estimators' : [170],\n",
    "    'num_leaves': [9],\n",
    "    'min_data_in_leaf': [10, 20, 50, 100],\n",
    "    'max_depth': [8, 9],\n",
    "    'learning_rate': [.1],\n",
    "    # 'bagging_freq': [3, 4, 5, 6, 7],\n",
    "    # 'bagging_fraction': np.linspace(0.6, 0.95, 10),\n",
    "    'reg_alpha': np.linspace(0.1, 0.95, 10),\n",
    "    'reg_lambda': np.linspace(0.1, 0.95, 10)}\n",
    "\n",
    "    \n",
    "scorer = make_scorer(mape, greater_is_better = False)   \n",
    "gbm_n2 = GridSearchCV(lgb.LGBMRegressor(cat_features= cat_var), \n",
    "                   param_grid, \n",
    "                   scoring = scorer, \n",
    "                   n_jobs = -2,\n",
    "                   cv = GroupKFold(n_splits=5))\n",
    "\n",
    "\n",
    "gbm_n2.fit(X_train_scaled, y_train, groups=groups)\n",
    "\n",
    "print(\"lightgbm mape:\", mape_by_month(y_test, gbm_n2.predict(X_test_scaled)), gbm_n2.best_params_)\n",
    "print(\"lightgbm rmse:\", rmse(y_test, gbm_n2.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "stacker= linear_model.LinearRegression()\n",
    "\n",
    "stacker.fit(pd.DataFrame({'col1': grid_xgb_result.predict(X_test_scaled),\n",
    "              'col2': gbm_g1.predict(X_test_scaled),\n",
    "               'col3': ridge_cv.predict(X_test_scaled)}),\n",
    "                X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = stacker.predict((pd.DataFrame({'col1': grid_xgb_result.predict(X_test_scaled),\n",
    "              'col2': gbm_g1.predict(X_test_scaled),\n",
    "               'col3': ridge_cv.predict(X_test_scaled)})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"stacker mape:\", mape_by_month(y_test, pred))\n",
    "print(\"stacker rmse:\", rmse(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mlxtend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mape: 77.13224507532261\n",
      "XGB rmse: 4326.8949875543485\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "stregr_1 = StackingRegressor(regressors=[gbm_g1_result, grid_xgb_result, svr], \n",
    "                           meta_regressor=ridge_cv)\n",
    "\n",
    "stregr_1.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"stregr_1 mape:\", mape_by_month(y_test, stregr_1.predict(X_test_scaled)))\n",
    "print(\"stregr_1 rmse:\", rmse(y_test, stregr_1.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stregr_1 mape: 77.15165151557427\n",
      "stregr_1 rmse: 4327.477517306288\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "stregr_2 = StackingRegressor(regressors=[gbm_g1_result, grid_xgb_result, svr], \n",
    "                             meta_regressor=ridge_cv,\n",
    "                             use_features_in_secondary=True)\n",
    "\n",
    "stregr_2.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"stregr_1_2 mape:\", mape_by_month(y_test, stregr_2.predict(X_test_scaled)))\n",
    "print(\"stregr_1_2 rmse:\", rmse(y_test, stregr_2.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w/o SVR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mape: 77.15165151557427\n",
      "XGB rmse: 4327.477517306288\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "stregr_2 = StackingRegressor(regressors=[gbm_g1_result, grid_xgb_result], \n",
    "                           meta_regressor=ridge_cv)\n",
    "\n",
    "stregr_2.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"stregr_2 mape:\", mape_by_month(y_test, stregr_2.predict(X_test_scaled)))\n",
    "print(\"stregr_2 rmse:\", rmse(y_test, stregr_2.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stregr_2 mape: 77.10670822781684\n",
      "stregr_2 rmse: 4327.203301896387\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "stregr_2 = StackingRegressor(regressors=[gbm_g1_result, grid_xgb_result], \n",
    "                             meta_regressor=ridge_cv,\n",
    "                             use_features_in_secondary=True)\n",
    "\n",
    "stregr_2.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"stregr_2 mape:\", mape_by_month(y_test, stregr_2.predict(X_test_scaled)))\n",
    "print(\"stregr_2 rmse:\", rmse(y_test, stregr_2.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### w/o SVR & use_features_in_secondary=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stregr_2 mape: 77.10670822781684\n",
      "stregr_2 rmse: 4327.203301896387\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "stregr_2 = StackingRegressor(regressors=[gbm_g1_result, grid_xgb_result], \n",
    "                            meta_regressor=ridge_cv)\n",
    "\n",
    "stregr_2.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"stregr_2 mape:\", mape_by_month(y_test, stregr_2.predict(X_test_scaled)))\n",
    "print(\"stregr_2 rmse:\", rmse(y_test, stregr_2.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stregr_3 mape: 76.49314308564793\n",
      "stregr_3 rmse: 4260.199986596819\n"
     ]
    }
   ],
   "source": [
    "stregr_3 = StackingRegressor(regressors=[ridge_cv, grid_xgb_result], \n",
    "                            meta_regressor=gbm_g1_result,\n",
    "                            use_features_in_secondary=True)\n",
    "\n",
    "stregr_3.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"stregr_3 mape:\", mape_by_month(y_test, stregr_3.predict(X_test_scaled)))\n",
    "print(\"stregr_3 rmse:\", rmse(y_test, stregr_3.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all models, but meta_regressor = lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stregr_3 mape: 76.4909991551237\n",
      "stregr_3 rmse: 4260.9201545274855\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "stregr_3 = StackingRegressor(regressors=[ridge_cv, grid_xgb_result, svr], \n",
    "                            meta_regressor=gbm_g1_result)\n",
    "\n",
    "stregr_3.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"stregr_3 mape:\", mape_by_month(y_test, stregr_3.predict(X_test_scaled)))\n",
    "print(\"stregr_3 rmse:\", rmse(y_test, stregr_3.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all models, but meta_regressor = lightgbm & use_features_in_secondary=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB mape: 76.42096805754505\n",
      "XGB rmse: 4258.359350488903\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.regressor import StackingRegressor\n",
    "\n",
    "stregr_3 = StackingRegressor(regressors=[ridge_cv, grid_xgb_result, svr], \n",
    "                            meta_regressor=gbm_g1_result,\n",
    "                            use_features_in_secondary=True)\n",
    "\n",
    "stregr_3.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"stregr_3 mape:\", mape_by_month(y_test, stregr_3.predict(X_test_scaled)))\n",
    "print(\"stregr_3 rmse:\", rmse(y_test, stregr_3.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TPOT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTRegressor\n",
    "from tpot.builtins import StackingEstimator\n",
    "from sklearn.pipeline import make_pipeline, make_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=450, style=ProgressStyle(descriptâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1 - Current best internal CV score: -1250958.359196122\n",
      "Generation 2 - Current best internal CV score: -1250958.359196122\n",
      "Generation 3 - Current best internal CV score: -1250958.359196122\n",
      "Generation 4 - Current best internal CV score: -1250958.359196122\n",
      "Generation 5 - Current best internal CV score: -1247403.7385499007\n",
      "Generation 6 - Current best internal CV score: -1246578.0061185411\n",
      "Generation 7 - Current best internal CV score: -1245702.6176878647\n",
      "Generation 8 - Current best internal CV score: -1245499.7739479125\n",
      "\n",
      "Best pipeline: ExtraTreesRegressor(ZeroCount(ElasticNetCV(LassoLarsCV(Normalizer(ElasticNetCV(input_matrix, l1_ratio=0.6000000000000001, tol=0.01), norm=l1), normalize=False), l1_ratio=0.6000000000000001, tol=0.01)), bootstrap=True, max_features=0.2, min_samples_leaf=10, min_samples_split=4, n_estimators=100)\n",
      "-19456399.769313164\n"
     ]
    }
   ],
   "source": [
    "tpot = TPOTRegressor(generations=8, population_size=50, verbosity=2)\n",
    "tpot.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"stregr_3 mape:\", mape_by_month(y_test, tpot.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpot mape: 78.09983868224177\n"
     ]
    }
   ],
   "source": [
    "print(\"tpot mape:\", mape_by_month(y_test, tpot.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use only StackingEstimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpot mape: 76.33902460458779\n"
     ]
    }
   ],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=ridge_cv),\n",
    "    gbm_g1_result)\n",
    "\n",
    "exported_pipeline.fit(X_train_scaled, y_train)\n",
    "results = exported_pipeline.predict(X_test_scaled)\n",
    "print(\"tpot mape:\", mape_by_month(y_test, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpot mape: 76.41617182539356\n"
     ]
    }
   ],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=grid_xgb_result),\n",
    "    gbm_g1_result)\n",
    "\n",
    "exported_pipeline.fit(X_train_scaled, y_train)\n",
    "results = exported_pipeline.predict(X_test_scaled)\n",
    "print(\"tpot mape:\", mape_by_month(y_test, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-632-12f93621d76d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     gbm_g1_result)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mexported_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexported_pipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_scaled\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tpot mape:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmape_by_month\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                 **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tpot\\builtins\\stacking_estimator.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \"\"\"\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    685\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1146\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1147\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1148\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    664\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 666\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    932\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 934\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    935\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    425\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=grid_xgb_result),\n",
    "    StackingEstimator(estimator=ridge_cv),\n",
    "    StackingEstimator(estimator=svr),\n",
    "    gbm_g1_result)\n",
    "\n",
    "exported_pipeline.fit(X_train_scaled, y_train)\n",
    "results = exported_pipeline.predict(X_test_scaled)\n",
    "print(\"tpot mape:\", mape_by_month(y_test, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpot mape: 76.1833432870904\n"
     ]
    }
   ],
   "source": [
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=ridge_cv),\n",
    "    StackingEstimator(estimator=grid_xgb_result),\n",
    "    gbm_g1_result)\n",
    "\n",
    "exported_pipeline.fit(X_train_scaled, y_train)\n",
    "results = exported_pipeline.predict(X_test_scaled)\n",
    "print(\"tpot mape:\", mape_by_month(y_test, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:587: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  if getattr(data, 'base', None) is not None and \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\xgboost\\core.py:588: FutureWarning: Series.base is deprecated and will be removed in a future version\n",
      "  data.base is not None and isinstance(data, np.ndarray) \\\n",
      "C:\\Users\\user\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tpot mape: 76.53996226486204\n"
     ]
    }
   ],
   "source": [
    "# 77.106 - StackingRegressor\n",
    "\n",
    "exported_pipeline = make_pipeline(\n",
    "    StackingEstimator(estimator=gbm_g1_result),\n",
    "    StackingEstimator(estimator=grid_xgb_result),\n",
    "    ridge_cv)\n",
    "\n",
    "exported_pipeline.fit(X_train_scaled, y_train)\n",
    "results = exported_pipeline.predict(X_test_scaled)\n",
    "print(\"tpot mape:\", mape_by_month(y_test, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.179501972823089\n",
      "2.233800624310626\n",
      "2641.4630718058074\n"
     ]
    }
   ],
   "source": [
    "test2 = test.copy()\n",
    "test2['y_pred'] = np.array(exported_pipeline.predict(X_test_scaled))\n",
    "test2['Customer_SKU'] = test2.merge(data_labels, left_index=True, right_index=True)['SKU_Customer']\n",
    "test2['y_pred'] = np.array(test2.groupby(['Customer_SKU', 'Year', 'Month_No', ]).y_pred.transform('sum'))\n",
    "\n",
    "final_data = test2[['Sales', 'y_pred', 'Customer_SKU', 'Year', 'Month_No']]\n",
    "final_data = final_data.drop_duplicates()\n",
    "\n",
    "\n",
    "final_data['MAD'] = np.where((final_data['y_pred'] == 0)&(final_data['Sales'] == 0), 0, \\\n",
    "                           np.where((final_data['y_pred'] == 0)&(final_data['Sales']!=0),1, np.abs(final_data['y_pred']-final_data['Sales'])/final_data['y_pred']))\n",
    " \n",
    "final_data['MAPE'] = np.where((final_data['y_pred'] == 0)&(final_data['Sales'] == 0), 0, \\\n",
    "                           np.where((final_data['y_pred'] != 0)&(final_data['Sales']==0),1, np.abs(final_data['y_pred']-final_data['Sales'])/final_data['Sales']))\n",
    "\n",
    "    \n",
    "print(final_data['MAD'].mean())   \n",
    "print(final_data['MAPE'].mean())     \n",
    "print(rmse_by_month(final_data['Sales'], final_data['y_pred']))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>SKU_Customer</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month_No</th>\n",
       "      <th>MAD</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>908.00</td>\n",
       "      <td>1355.940948</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.330354</td>\n",
       "      <td>0.493327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>884.00</td>\n",
       "      <td>1307.660955</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.323984</td>\n",
       "      <td>0.479254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1433.40</td>\n",
       "      <td>1344.623901</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.066023</td>\n",
       "      <td>0.061934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>971.75</td>\n",
       "      <td>1327.359232</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.267907</td>\n",
       "      <td>0.365947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>1041.75</td>\n",
       "      <td>1335.907865</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.220193</td>\n",
       "      <td>0.282369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14962</td>\n",
       "      <td>5861.00</td>\n",
       "      <td>3817.674067</td>\n",
       "      <td>WALMART US19200-79329</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.535228</td>\n",
       "      <td>0.348631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14967</td>\n",
       "      <td>9319.00</td>\n",
       "      <td>3800.409398</td>\n",
       "      <td>WALMART US19200-79329</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1.452104</td>\n",
       "      <td>0.592187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14971</td>\n",
       "      <td>835.00</td>\n",
       "      <td>3808.958031</td>\n",
       "      <td>WALMART US19200-79329</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.780780</td>\n",
       "      <td>3.561626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14975</td>\n",
       "      <td>3230.60</td>\n",
       "      <td>3845.920978</td>\n",
       "      <td>WALMART US19200-79329</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.159993</td>\n",
       "      <td>0.190466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14980</td>\n",
       "      <td>554.50</td>\n",
       "      <td>3743.449278</td>\n",
       "      <td>WALMART US19200-79329</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.851875</td>\n",
       "      <td>5.751036</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>336 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sales       y_pred                SKU_Customer  Year  Month_No  \\\n",
       "198     908.00  1355.940948  ALL OTHERS - US62338-91101  2018        10   \n",
       "200     884.00  1307.660955  ALL OTHERS - US62338-91101  2018        11   \n",
       "204    1433.40  1344.623901  ALL OTHERS - US62338-91101  2018        12   \n",
       "209     971.75  1327.359232  ALL OTHERS - US62338-91101  2019         1   \n",
       "213    1041.75  1335.907865  ALL OTHERS - US62338-91101  2019         2   \n",
       "...        ...          ...                         ...   ...       ...   \n",
       "14962  5861.00  3817.674067       WALMART US19200-79329  2018        12   \n",
       "14967  9319.00  3800.409398       WALMART US19200-79329  2019         1   \n",
       "14971   835.00  3808.958031       WALMART US19200-79329  2019         2   \n",
       "14975  3230.60  3845.920978       WALMART US19200-79329  2019         3   \n",
       "14980   554.50  3743.449278       WALMART US19200-79329  2019         4   \n",
       "\n",
       "            MAD      MAPE  \n",
       "198    0.330354  0.493327  \n",
       "200    0.323984  0.479254  \n",
       "204    0.066023  0.061934  \n",
       "209    0.267907  0.365947  \n",
       "213    0.220193  0.282369  \n",
       "...         ...       ...  \n",
       "14962  0.535228  0.348631  \n",
       "14967  1.452104  0.592187  \n",
       "14971  0.780780  3.561626  \n",
       "14975  0.159993  0.190466  \n",
       "14980  0.851875  5.751036  \n",
       "\n",
       "[336 rows x 7 columns]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('Final_result_for_weeks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.read_csv('Final_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x25280a17dd8>"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(final_data['Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sales</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>SKU_Customer</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month_No</th>\n",
       "      <th>MAD</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>908.00</td>\n",
       "      <td>1355.940948</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.330354</td>\n",
       "      <td>0.493327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>200</td>\n",
       "      <td>884.00</td>\n",
       "      <td>1307.660955</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.323984</td>\n",
       "      <td>0.479254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>204</td>\n",
       "      <td>1433.40</td>\n",
       "      <td>1344.623901</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.066023</td>\n",
       "      <td>0.061934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>209</td>\n",
       "      <td>971.75</td>\n",
       "      <td>1327.359232</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.267907</td>\n",
       "      <td>0.365947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>213</td>\n",
       "      <td>1041.75</td>\n",
       "      <td>1335.907865</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.220193</td>\n",
       "      <td>0.282369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    Sales       y_pred                SKU_Customer  Year  \\\n",
       "0         198   908.00  1355.940948  ALL OTHERS - US62338-91101  2018   \n",
       "1         200   884.00  1307.660955  ALL OTHERS - US62338-91101  2018   \n",
       "2         204  1433.40  1344.623901  ALL OTHERS - US62338-91101  2018   \n",
       "3         209   971.75  1327.359232  ALL OTHERS - US62338-91101  2019   \n",
       "4         213  1041.75  1335.907865  ALL OTHERS - US62338-91101  2019   \n",
       "\n",
       "   Month_No       MAD      MAPE  \n",
       "0        10  0.330354  0.493327  \n",
       "1        11  0.323984  0.479254  \n",
       "2        12  0.066023  0.061934  \n",
       "3         1  0.267907  0.365947  \n",
       "4         2  0.220193  0.282369  "
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2528394b400>"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(final_data['Sales'])\n",
    "sns.kdeplot(final_data['y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6068816887333172"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[final_data['Sales']>2000].MAD.mean()\n",
    "final_data[final_data['Sales']<2000].MAD.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2772333413003364\n",
      "2.805362811434578\n"
     ]
    }
   ],
   "source": [
    "print(final_data[final_data['Sales']>1000].MAPE.mean())\n",
    "print(final_data[final_data['Sales']<1000].MAPE.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sales</th>\n",
       "      <th>y_pred</th>\n",
       "      <th>SKU_Customer</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month_No</th>\n",
       "      <th>MAD</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>1433.40</td>\n",
       "      <td>1344.623901</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.066023</td>\n",
       "      <td>0.061934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>1041.75</td>\n",
       "      <td>1335.907865</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.220193</td>\n",
       "      <td>0.282369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>1023.80</td>\n",
       "      <td>1372.870812</td>\n",
       "      <td>ALL OTHERS - US62338-91101</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.254263</td>\n",
       "      <td>0.340956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>1648.50</td>\n",
       "      <td>1720.691652</td>\n",
       "      <td>ALL OTHERS - US62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.041955</td>\n",
       "      <td>0.043792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>514</td>\n",
       "      <td>1587.50</td>\n",
       "      <td>1672.411659</td>\n",
       "      <td>ALL OTHERS - US62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.050772</td>\n",
       "      <td>0.053488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>518</td>\n",
       "      <td>1372.00</td>\n",
       "      <td>1709.374605</td>\n",
       "      <td>ALL OTHERS - US62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.197367</td>\n",
       "      <td>0.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>523</td>\n",
       "      <td>1527.75</td>\n",
       "      <td>1692.109936</td>\n",
       "      <td>ALL OTHERS - US62338-92944</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.097133</td>\n",
       "      <td>0.107583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>527</td>\n",
       "      <td>1004.25</td>\n",
       "      <td>1700.658569</td>\n",
       "      <td>ALL OTHERS - US62338-92944</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.409493</td>\n",
       "      <td>0.693461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>531</td>\n",
       "      <td>1280.60</td>\n",
       "      <td>1737.621515</td>\n",
       "      <td>ALL OTHERS - US62338-92944</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.263016</td>\n",
       "      <td>0.356881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845</td>\n",
       "      <td>1980.80</td>\n",
       "      <td>433.093859</td>\n",
       "      <td>ALL OTHERS - US62338-99058</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>3.573604</td>\n",
       "      <td>0.781354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2396</td>\n",
       "      <td>1520.00</td>\n",
       "      <td>948.860714</td>\n",
       "      <td>DOLLAR GENERAL62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.601921</td>\n",
       "      <td>0.375750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2398</td>\n",
       "      <td>1496.25</td>\n",
       "      <td>850.666543</td>\n",
       "      <td>DOLLAR GENERAL62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.758915</td>\n",
       "      <td>0.431468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2402</td>\n",
       "      <td>1204.60</td>\n",
       "      <td>822.651496</td>\n",
       "      <td>DOLLAR GENERAL62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.464290</td>\n",
       "      <td>0.317075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3024</td>\n",
       "      <td>2149.00</td>\n",
       "      <td>587.840448</td>\n",
       "      <td>DRUGSTORE62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>2.655754</td>\n",
       "      <td>0.726459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3026</td>\n",
       "      <td>4449.50</td>\n",
       "      <td>509.105906</td>\n",
       "      <td>DRUGSTORE62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>7.739832</td>\n",
       "      <td>0.885581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8054</td>\n",
       "      <td>1152.40</td>\n",
       "      <td>1074.483778</td>\n",
       "      <td>TARGET62338-99058</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.072515</td>\n",
       "      <td>0.067612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8067</td>\n",
       "      <td>1072.00</td>\n",
       "      <td>1129.626588</td>\n",
       "      <td>TARGET62338-99058</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.051014</td>\n",
       "      <td>0.053756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8676</td>\n",
       "      <td>1987.50</td>\n",
       "      <td>1985.799547</td>\n",
       "      <td>WALMART US62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8678</td>\n",
       "      <td>1837.75</td>\n",
       "      <td>1699.434295</td>\n",
       "      <td>WALMART US62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.081389</td>\n",
       "      <td>0.075264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8682</td>\n",
       "      <td>1686.40</td>\n",
       "      <td>1457.249969</td>\n",
       "      <td>WALMART US62338-92944</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.157248</td>\n",
       "      <td>0.135881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8687</td>\n",
       "      <td>2118.25</td>\n",
       "      <td>1564.488075</td>\n",
       "      <td>WALMART US62338-92944</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.353957</td>\n",
       "      <td>0.261424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8691</td>\n",
       "      <td>1730.75</td>\n",
       "      <td>1369.766141</td>\n",
       "      <td>WALMART US62338-92944</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.263537</td>\n",
       "      <td>0.208571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8695</td>\n",
       "      <td>1724.80</td>\n",
       "      <td>1593.184298</td>\n",
       "      <td>WALMART US62338-92944</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.082612</td>\n",
       "      <td>0.076308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1325.50</td>\n",
       "      <td>1454.457709</td>\n",
       "      <td>WALMART US62338-92944</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.088664</td>\n",
       "      <td>0.097290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8990</td>\n",
       "      <td>2942.00</td>\n",
       "      <td>2913.516138</td>\n",
       "      <td>WALMART US62338-99058</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.009776</td>\n",
       "      <td>0.009682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8992</td>\n",
       "      <td>2497.25</td>\n",
       "      <td>2861.980094</td>\n",
       "      <td>WALMART US62338-99058</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.127440</td>\n",
       "      <td>0.146053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8996</td>\n",
       "      <td>2384.80</td>\n",
       "      <td>2659.147461</td>\n",
       "      <td>WALMART US62338-99058</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.103171</td>\n",
       "      <td>0.115040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9001</td>\n",
       "      <td>2962.25</td>\n",
       "      <td>2651.142856</td>\n",
       "      <td>WALMART US62338-99058</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.117348</td>\n",
       "      <td>0.105024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9005</td>\n",
       "      <td>2663.25</td>\n",
       "      <td>2633.571525</td>\n",
       "      <td>WALMART US62338-99058</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.011269</td>\n",
       "      <td>0.011144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9009</td>\n",
       "      <td>2652.40</td>\n",
       "      <td>2906.099439</td>\n",
       "      <td>WALMART US62338-99058</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.087299</td>\n",
       "      <td>0.095649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9014</td>\n",
       "      <td>2274.00</td>\n",
       "      <td>2761.941726</td>\n",
       "      <td>WALMART US62338-99058</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.176666</td>\n",
       "      <td>0.214574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9304</td>\n",
       "      <td>3699.50</td>\n",
       "      <td>4185.838930</td>\n",
       "      <td>ALL OTHERS - US51700-77050</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.116187</td>\n",
       "      <td>0.131461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9306</td>\n",
       "      <td>3182.75</td>\n",
       "      <td>4137.558937</td>\n",
       "      <td>ALL OTHERS - US51700-77050</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.230766</td>\n",
       "      <td>0.299995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9310</td>\n",
       "      <td>5027.40</td>\n",
       "      <td>4174.521883</td>\n",
       "      <td>ALL OTHERS - US51700-77050</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.204306</td>\n",
       "      <td>0.169646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9315</td>\n",
       "      <td>3130.00</td>\n",
       "      <td>4157.257214</td>\n",
       "      <td>ALL OTHERS - US51700-77050</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.247100</td>\n",
       "      <td>0.328197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9319</td>\n",
       "      <td>3861.25</td>\n",
       "      <td>4165.805847</td>\n",
       "      <td>ALL OTHERS - US51700-77050</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.073109</td>\n",
       "      <td>0.078875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9323</td>\n",
       "      <td>4284.60</td>\n",
       "      <td>4202.768794</td>\n",
       "      <td>ALL OTHERS - US51700-77050</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.019471</td>\n",
       "      <td>0.019099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9328</td>\n",
       "      <td>2269.00</td>\n",
       "      <td>4100.297094</td>\n",
       "      <td>ALL OTHERS - US51700-77050</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.446625</td>\n",
       "      <td>0.807094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10880</td>\n",
       "      <td>1500.80</td>\n",
       "      <td>881.324553</td>\n",
       "      <td>KROGER51700-77050</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.702891</td>\n",
       "      <td>0.412763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11816</td>\n",
       "      <td>2845.00</td>\n",
       "      <td>3087.112347</td>\n",
       "      <td>WALMART US51700-77050</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.078427</td>\n",
       "      <td>0.085101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11818</td>\n",
       "      <td>2896.75</td>\n",
       "      <td>3128.892558</td>\n",
       "      <td>WALMART US51700-77050</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.074193</td>\n",
       "      <td>0.080139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11822</td>\n",
       "      <td>2906.00</td>\n",
       "      <td>3169.340551</td>\n",
       "      <td>WALMART US51700-77050</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.083090</td>\n",
       "      <td>0.090620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11827</td>\n",
       "      <td>3201.75</td>\n",
       "      <td>3300.766460</td>\n",
       "      <td>WALMART US51700-77050</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.029998</td>\n",
       "      <td>0.030926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11831</td>\n",
       "      <td>2841.75</td>\n",
       "      <td>3281.084063</td>\n",
       "      <td>WALMART US51700-77050</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133899</td>\n",
       "      <td>0.154600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11835</td>\n",
       "      <td>3076.20</td>\n",
       "      <td>3208.388051</td>\n",
       "      <td>WALMART US51700-77050</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.041201</td>\n",
       "      <td>0.042971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11840</td>\n",
       "      <td>2070.50</td>\n",
       "      <td>2993.599701</td>\n",
       "      <td>WALMART US51700-77050</td>\n",
       "      <td>2019</td>\n",
       "      <td>4</td>\n",
       "      <td>0.308358</td>\n",
       "      <td>0.445834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12130</td>\n",
       "      <td>1657.00</td>\n",
       "      <td>1987.576314</td>\n",
       "      <td>ALL OTHERS - US19200-79329</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.166321</td>\n",
       "      <td>0.199503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12132</td>\n",
       "      <td>1318.00</td>\n",
       "      <td>1939.296321</td>\n",
       "      <td>ALL OTHERS - US19200-79329</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.320372</td>\n",
       "      <td>0.471393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12136</td>\n",
       "      <td>2183.00</td>\n",
       "      <td>1976.259267</td>\n",
       "      <td>ALL OTHERS - US19200-79329</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.104612</td>\n",
       "      <td>0.094705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12141</td>\n",
       "      <td>1561.75</td>\n",
       "      <td>1958.994598</td>\n",
       "      <td>ALL OTHERS - US19200-79329</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.202780</td>\n",
       "      <td>0.254359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12145</td>\n",
       "      <td>2035.50</td>\n",
       "      <td>1967.543231</td>\n",
       "      <td>ALL OTHERS - US19200-79329</td>\n",
       "      <td>2019</td>\n",
       "      <td>2</td>\n",
       "      <td>0.034539</td>\n",
       "      <td>0.033386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12149</td>\n",
       "      <td>2387.40</td>\n",
       "      <td>2004.506177</td>\n",
       "      <td>ALL OTHERS - US19200-79329</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.191017</td>\n",
       "      <td>0.160381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12760</td>\n",
       "      <td>1200.25</td>\n",
       "      <td>724.293886</td>\n",
       "      <td>DRUGSTORE19200-79329</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.657131</td>\n",
       "      <td>0.396547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12764</td>\n",
       "      <td>1661.20</td>\n",
       "      <td>761.256832</td>\n",
       "      <td>DRUGSTORE19200-79329</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>1.182181</td>\n",
       "      <td>0.541743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12769</td>\n",
       "      <td>1214.75</td>\n",
       "      <td>743.992163</td>\n",
       "      <td>DRUGSTORE19200-79329</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>0.632746</td>\n",
       "      <td>0.387535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14956</td>\n",
       "      <td>1302.00</td>\n",
       "      <td>3828.991114</td>\n",
       "      <td>WALMART US19200-79329</td>\n",
       "      <td>2018</td>\n",
       "      <td>10</td>\n",
       "      <td>0.659963</td>\n",
       "      <td>1.940853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14958</td>\n",
       "      <td>2577.50</td>\n",
       "      <td>3780.711121</td>\n",
       "      <td>WALMART US19200-79329</td>\n",
       "      <td>2018</td>\n",
       "      <td>11</td>\n",
       "      <td>0.318250</td>\n",
       "      <td>0.466813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14962</td>\n",
       "      <td>5861.00</td>\n",
       "      <td>3817.674067</td>\n",
       "      <td>WALMART US19200-79329</td>\n",
       "      <td>2018</td>\n",
       "      <td>12</td>\n",
       "      <td>0.535228</td>\n",
       "      <td>0.348631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14967</td>\n",
       "      <td>9319.00</td>\n",
       "      <td>3800.409398</td>\n",
       "      <td>WALMART US19200-79329</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "      <td>1.452104</td>\n",
       "      <td>0.592187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14975</td>\n",
       "      <td>3230.60</td>\n",
       "      <td>3845.920978</td>\n",
       "      <td>WALMART US19200-79329</td>\n",
       "      <td>2019</td>\n",
       "      <td>3</td>\n",
       "      <td>0.159993</td>\n",
       "      <td>0.190466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sales       y_pred                SKU_Customer  Year  Month_No  \\\n",
       "204    1433.40  1344.623901  ALL OTHERS - US62338-91101  2018        12   \n",
       "213    1041.75  1335.907865  ALL OTHERS - US62338-91101  2019         2   \n",
       "217    1023.80  1372.870812  ALL OTHERS - US62338-91101  2019         3   \n",
       "512    1648.50  1720.691652  ALL OTHERS - US62338-92944  2018        10   \n",
       "514    1587.50  1672.411659  ALL OTHERS - US62338-92944  2018        11   \n",
       "518    1372.00  1709.374605  ALL OTHERS - US62338-92944  2018        12   \n",
       "523    1527.75  1692.109936  ALL OTHERS - US62338-92944  2019         1   \n",
       "527    1004.25  1700.658569  ALL OTHERS - US62338-92944  2019         2   \n",
       "531    1280.60  1737.621515  ALL OTHERS - US62338-92944  2019         3   \n",
       "845    1980.80   433.093859  ALL OTHERS - US62338-99058  2019         3   \n",
       "2396   1520.00   948.860714   DOLLAR GENERAL62338-92944  2018        10   \n",
       "2398   1496.25   850.666543   DOLLAR GENERAL62338-92944  2018        11   \n",
       "2402   1204.60   822.651496   DOLLAR GENERAL62338-92944  2018        12   \n",
       "3024   2149.00   587.840448        DRUGSTORE62338-92944  2018        10   \n",
       "3026   4449.50   509.105906        DRUGSTORE62338-92944  2018        11   \n",
       "8054   1152.40  1074.483778           TARGET62338-99058  2018        12   \n",
       "8067   1072.00  1129.626588           TARGET62338-99058  2019         3   \n",
       "8676   1987.50  1985.799547       WALMART US62338-92944  2018        10   \n",
       "8678   1837.75  1699.434295       WALMART US62338-92944  2018        11   \n",
       "8682   1686.40  1457.249969       WALMART US62338-92944  2018        12   \n",
       "8687   2118.25  1564.488075       WALMART US62338-92944  2019         1   \n",
       "8691   1730.75  1369.766141       WALMART US62338-92944  2019         2   \n",
       "8695   1724.80  1593.184298       WALMART US62338-92944  2019         3   \n",
       "8700   1325.50  1454.457709       WALMART US62338-92944  2019         4   \n",
       "8990   2942.00  2913.516138       WALMART US62338-99058  2018        10   \n",
       "8992   2497.25  2861.980094       WALMART US62338-99058  2018        11   \n",
       "8996   2384.80  2659.147461       WALMART US62338-99058  2018        12   \n",
       "9001   2962.25  2651.142856       WALMART US62338-99058  2019         1   \n",
       "9005   2663.25  2633.571525       WALMART US62338-99058  2019         2   \n",
       "9009   2652.40  2906.099439       WALMART US62338-99058  2019         3   \n",
       "9014   2274.00  2761.941726       WALMART US62338-99058  2019         4   \n",
       "9304   3699.50  4185.838930  ALL OTHERS - US51700-77050  2018        10   \n",
       "9306   3182.75  4137.558937  ALL OTHERS - US51700-77050  2018        11   \n",
       "9310   5027.40  4174.521883  ALL OTHERS - US51700-77050  2018        12   \n",
       "9315   3130.00  4157.257214  ALL OTHERS - US51700-77050  2019         1   \n",
       "9319   3861.25  4165.805847  ALL OTHERS - US51700-77050  2019         2   \n",
       "9323   4284.60  4202.768794  ALL OTHERS - US51700-77050  2019         3   \n",
       "9328   2269.00  4100.297094  ALL OTHERS - US51700-77050  2019         4   \n",
       "10880  1500.80   881.324553           KROGER51700-77050  2018        12   \n",
       "11816  2845.00  3087.112347       WALMART US51700-77050  2018        10   \n",
       "11818  2896.75  3128.892558       WALMART US51700-77050  2018        11   \n",
       "11822  2906.00  3169.340551       WALMART US51700-77050  2018        12   \n",
       "11827  3201.75  3300.766460       WALMART US51700-77050  2019         1   \n",
       "11831  2841.75  3281.084063       WALMART US51700-77050  2019         2   \n",
       "11835  3076.20  3208.388051       WALMART US51700-77050  2019         3   \n",
       "11840  2070.50  2993.599701       WALMART US51700-77050  2019         4   \n",
       "12130  1657.00  1987.576314  ALL OTHERS - US19200-79329  2018        10   \n",
       "12132  1318.00  1939.296321  ALL OTHERS - US19200-79329  2018        11   \n",
       "12136  2183.00  1976.259267  ALL OTHERS - US19200-79329  2018        12   \n",
       "12141  1561.75  1958.994598  ALL OTHERS - US19200-79329  2019         1   \n",
       "12145  2035.50  1967.543231  ALL OTHERS - US19200-79329  2019         2   \n",
       "12149  2387.40  2004.506177  ALL OTHERS - US19200-79329  2019         3   \n",
       "12760  1200.25   724.293886        DRUGSTORE19200-79329  2018        11   \n",
       "12764  1661.20   761.256832        DRUGSTORE19200-79329  2018        12   \n",
       "12769  1214.75   743.992163        DRUGSTORE19200-79329  2019         1   \n",
       "14956  1302.00  3828.991114       WALMART US19200-79329  2018        10   \n",
       "14958  2577.50  3780.711121       WALMART US19200-79329  2018        11   \n",
       "14962  5861.00  3817.674067       WALMART US19200-79329  2018        12   \n",
       "14967  9319.00  3800.409398       WALMART US19200-79329  2019         1   \n",
       "14975  3230.60  3845.920978       WALMART US19200-79329  2019         3   \n",
       "\n",
       "            MAD      MAPE  \n",
       "204    0.066023  0.061934  \n",
       "213    0.220193  0.282369  \n",
       "217    0.254263  0.340956  \n",
       "512    0.041955  0.043792  \n",
       "514    0.050772  0.053488  \n",
       "518    0.197367  0.245900  \n",
       "523    0.097133  0.107583  \n",
       "527    0.409493  0.693461  \n",
       "531    0.263016  0.356881  \n",
       "845    3.573604  0.781354  \n",
       "2396   0.601921  0.375750  \n",
       "2398   0.758915  0.431468  \n",
       "2402   0.464290  0.317075  \n",
       "3024   2.655754  0.726459  \n",
       "3026   7.739832  0.885581  \n",
       "8054   0.072515  0.067612  \n",
       "8067   0.051014  0.053756  \n",
       "8676   0.000856  0.000856  \n",
       "8678   0.081389  0.075264  \n",
       "8682   0.157248  0.135881  \n",
       "8687   0.353957  0.261424  \n",
       "8691   0.263537  0.208571  \n",
       "8695   0.082612  0.076308  \n",
       "8700   0.088664  0.097290  \n",
       "8990   0.009776  0.009682  \n",
       "8992   0.127440  0.146053  \n",
       "8996   0.103171  0.115040  \n",
       "9001   0.117348  0.105024  \n",
       "9005   0.011269  0.011144  \n",
       "9009   0.087299  0.095649  \n",
       "9014   0.176666  0.214574  \n",
       "9304   0.116187  0.131461  \n",
       "9306   0.230766  0.299995  \n",
       "9310   0.204306  0.169646  \n",
       "9315   0.247100  0.328197  \n",
       "9319   0.073109  0.078875  \n",
       "9323   0.019471  0.019099  \n",
       "9328   0.446625  0.807094  \n",
       "10880  0.702891  0.412763  \n",
       "11816  0.078427  0.085101  \n",
       "11818  0.074193  0.080139  \n",
       "11822  0.083090  0.090620  \n",
       "11827  0.029998  0.030926  \n",
       "11831  0.133899  0.154600  \n",
       "11835  0.041201  0.042971  \n",
       "11840  0.308358  0.445834  \n",
       "12130  0.166321  0.199503  \n",
       "12132  0.320372  0.471393  \n",
       "12136  0.104612  0.094705  \n",
       "12141  0.202780  0.254359  \n",
       "12145  0.034539  0.033386  \n",
       "12149  0.191017  0.160381  \n",
       "12760  0.657131  0.396547  \n",
       "12764  1.182181  0.541743  \n",
       "12769  0.632746  0.387535  \n",
       "14956  0.659963  1.940853  \n",
       "14958  0.318250  0.466813  \n",
       "14962  0.535228  0.348631  \n",
       "14967  1.452104  0.592187  \n",
       "14975  0.159993  0.190466  "
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[final_data['Sales']>1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.sort_values(by='Sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['Sales_perc'] = round(100*final_data.Sales.cumsum(axis = 0) /final_data.Sales.sum(),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test.copy()\n",
    "test2['y_pred'] = np.array(gbm.predict(X_test_scaled))\n",
    "test2['Customer_SKU'] = test2.merge(data_labels, left_index=True, right_index=True)['SKU_Customer']\n",
    "test2['y_pred'] = np.array(test2.groupby(['Customer_SKU', 'Year', 'Month_No', ]).y_pred.transform('sum'))\n",
    "\n",
    "final_data = test2[['Sales', 'y_pred', 'Customer_SKU', 'Year', 'Month_No']]\n",
    "final_data = final_data.drop_duplicates()\n",
    "\n",
    "\n",
    "final_data['MAD'] = np.where((final_data['y_pred'] == 0)&(final_data['Sales'] == 0), 0, \\\n",
    "                           np.where((final_data['y_pred'] == 0)&(final_data['Sales']!=0),1, np.abs(final_data['y_pred']-final_data['Sales'])/final_data['y_pred']))\n",
    " \n",
    "final_data['MAPE'] = np.where((final_data['y_pred'] == 0)&(final_data['Sales'] == 0), 0, \\\n",
    "                           np.where((final_data['y_pred'] != 0)&(final_data['Sales']==0),1, np.abs(final_data['y_pred']-final_data['Sales'])/final_data['Sales']))\n",
    "\n",
    "    \n",
    "print(final_data['MAD'].mean())   \n",
    "print(final_data['MAPE'].mean())     \n",
    "print(rmse_by_month(final_data['Sales'], final_data['y_pred']))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = test.copy()\n",
    "test2['y_pred'] = np.array(ridge_cv.predict(X_test_scaled))\n",
    "test2['SKU_Customer'] = test2.merge(data_labels, left_index=True, right_index=True)['SKU_Customer']\n",
    "\n",
    "final_data = test2[['Sales', 'y_pred', 'SKU_Customer', 'Year', 'Month_No']]\n",
    "final_data = final_data.drop_duplicates()\n",
    "\n",
    "\n",
    "final_data['MAD'] = np.where((final_data['y_pred'] == 0)&(final_data['Sales'] == 0), 0, \\\n",
    "                           np.where((final_data['y_pred'] == 0)&(final_data['Sales']!=0),1, np.abs(final_data['y_pred']-final_data['Sales'])/final_data['y_pred']))\n",
    " \n",
    "final_data['MAPE'] = np.where((final_data['y_pred'] == 0)&(final_data['Sales'] == 0), 0, \\\n",
    "                           np.where((final_data['y_pred'] != 0)&(final_data['Sales']==0),1, np.abs(final_data['y_pred']-final_data['Sales'])/final_data['Sales']))\n",
    "\n",
    "    \n",
    "print(final_data['MAD'].mean())   \n",
    "print(final_data['MAPE'].mean())     \n",
    "print(rmse_by_month(final_data['Sales'], final_data['y_pred']) )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
